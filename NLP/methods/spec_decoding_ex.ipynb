{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "big_model_path = r\"../../../models/SmolLM2_360m\"\n",
    "small_model_path = r\"../../../models/SmolLM2-135M-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "small_model = AutoModelForCausalLM.from_pretrained(small_model_path, device_map=device)\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(small_model_path, device_map=device)\n",
    "\n",
    "big_model = AutoModelForCausalLM.from_pretrained(big_model_path, device_map=device)\n",
    "big_tokenizer = AutoTokenizer.from_pretrained(big_model_path, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_inference(big_model, big_tokenizer, prompt, max_new_tokens=500):\n",
    "    inputs = big_tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = big_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    return big_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "normal_inference(big_model, big_tokenizer, prompt=\"hello my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_decoding(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=500):\n",
    "    # Step 1: Use the small model to generate the draft\n",
    "    inputs = small_tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    small_outputs = small_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    draft = small_tokenizer.decode(small_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    big_inputs = big_tokenizer(draft, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = big_model(big_inputs['input_ids'])\n",
    "        log_probs = torch.log_softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    draft_token_ids = big_inputs['input_ids']\n",
    "    log_likelihood = 0\n",
    "    for i in range(draft_token_ids.size(1) - 1):\n",
    "        token_id = draft_token_ids[0, i + 1]\n",
    "        log_likelihood += log_probs[0, i, token_id].item()\n",
    "\n",
    "    avg_log_likelihood = log_likelihood / (draft_token_ids.size(1) - 1)\n",
    "\n",
    "    return draft, avg_log_likelihood\n",
    "\n",
    "speculative_decoding(small_model, big_model, small_tokenizer, big_tokenizer, prompt=\"hello my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_latency(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=500):\n",
    "    start_time = time.time()\n",
    "    normal_output = normal_inference(big_model, big_tokenizer, prompt, max_new_tokens)\n",
    "    normal_inference_latency = time.time() - start_time\n",
    "    print(f\"Normal Inference Output: {normal_output}\")\n",
    "    print(f\"Normal Inference Latency: {normal_inference_latency:.4f} seconds\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    speculative_output, log_likelihood = speculative_decoding(\n",
    "        small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens\n",
    "    )\n",
    "    speculative_decoding_latency = time.time() - start_time\n",
    "    print(f\"Speculative Decoding Output: {speculative_output}\")\n",
    "    print(f\"Speculative Decoding Latency: {speculative_decoding_latency:.4f} seconds\")\n",
    "    print(f\"Log Likelihood (Verification Score): {log_likelihood:.4f}\")\n",
    "\n",
    "    return normal_inference_latency, speculative_decoding_latency\n",
    "\n",
    "\n",
    "measure_latency(small_model, big_model, small_tokenizer, big_tokenizer, prompt=\"Write a short story about Daniel. Hello my name is Danieal and \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Inference Output: Write a short story about Daniel. Hello my name is Danieal and Â I am a 12 year old boy who loves to read and write. I am a very creative person and I love to draw and write. I am a very good student and I love to learn new things. I am very good at math and I love to learn new things. I am very good at sports and I love to play them. I am very good at music and I love to play it. I am very good at drawing and I love to draw. I am very good at writing and I love to write. I am very good at cooking and I love to cook. I am very good at math and I love to learn new things. I am very good at sports and I love to play them. I am very good at music and I love to play it. I am very good at drawing and I love to draw. I am very good at writing and I love to write. I am very good at cooking and I love to cook. I am very good at math and I love to learn new things. I am very good at sports and I love to play them. I am very good at music and I love to play it. I am very good at drawing and I love to draw. I am very good at writing and I love to write. I am very good at cooking and I love to cook. I am very good at math and I love to learn new things. I am very good at sports and I love to play them. I am very good at music and I love to play it. I am very good at drawing and I love to draw. I am very good at writing and I love to write. I am very good at cooking and I love to cook. I am very good at math and I love to learn new things. I am very good at sports and I love to play them. I am very good at music and I love to play it. I am very good at drawing and I love to draw. I am very good at writing and I love to write. I am very good at cooking and I love to cook. I am very good at math and I love to learn new things. I am very good at sports and I love to play them. I am very good at music and I love to play it. I am very good at drawing and I love to draw. I am very good at writing and I love to write. I am very good at cooking and I love to cook\n",
      "Normal Inference Latency: 13.6760 seconds\n",
      "\n",
      "\n",
      "\n",
      "Speculative Decoding Output: Write a short story about Daniel. Hello my name is Danieal and  I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy. I am a 16 year old boy.\n",
      "Speculative Decoding Latency: 14.0326 seconds\n",
      "Log Likelihood (Verification Score): -0.1878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13.676013946533203, 14.032550811767578)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
