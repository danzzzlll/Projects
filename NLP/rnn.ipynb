{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from base import BaseRnnFamily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(BaseRnnFamily):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, loss_type=\"mse\", **kwargs):\n",
    "        super().__init__(input_dim, hidden_dim, output_dim, loss_type=loss_type, **kwargs)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.h_t = np.zeros((self.hidden_dim,))\n",
    "        self.states = {}\n",
    "\n",
    "    def init_weights(self, **kwargs):\n",
    "        self.W_xh = np.random.uniform(0, 1, (self.input_dim, self.hidden_dim))\n",
    "        self.W_hh = np.random.uniform(0, 1, (self.hidden_dim, self.hidden_dim))\n",
    "        self.b_h = np.random.uniform(0, 1, self.hidden_dim)\n",
    "        self.W_hy = np.random.uniform(0, 1, (self.hidden_dim, self.output_dim))\n",
    "        self.b_y = np.random.uniform(0, 1, self.output_dim)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def forward_step(self, x_t, h_prev):\n",
    "        x_latent = x_t @ self.W_xh\n",
    "        h_relative = h_prev @ self.W_hh\n",
    "        x_interm = x_latent + h_relative + self.b_h\n",
    "        h_t = self.tanh(x_interm)\n",
    "        y_t = h_t @ self.W_hy + self.b_y\n",
    "        return h_t, y_t\n",
    "\n",
    "    def forward_sequence(self, x_seq):\n",
    "        hidden_states = []\n",
    "        y_states = []\n",
    "        h_prev = self.h_t\n",
    "\n",
    "        for x_t in x_seq:\n",
    "            h_t, y_t = self.forward_step(x_t, h_prev)\n",
    "            h_prev = h_t\n",
    "            hidden_states.append(h_t.copy())\n",
    "            y_states.append(y_t.copy())\n",
    "\n",
    "        self.states['hidden_states'] = np.array(hidden_states)\n",
    "        self.states['y_states'] = np.array(y_states)\n",
    "        return np.array(y_states)\n",
    "\n",
    "    def backward_step(self, grad_output, x_t, h_t, h_prev):\n",
    "        dW_hy = np.outer(h_t, grad_output)  # (hidden_dim, output_dim)\n",
    "        db_y = grad_output  # (output_dim,)\n",
    "\n",
    "        grad_h = grad_output @ self.W_hy.T  # (hidden_dim,)\n",
    "        dtanh = 1 - h_t ** 2\n",
    "        grad_h_raw = grad_h * dtanh  # (hidden_dim,)\n",
    "\n",
    "        dW_xh = np.outer(x_t, grad_h_raw)  # (input_dim, hidden_dim)\n",
    "        dW_hh = np.outer(h_prev, grad_h_raw)  # (hidden_dim, hidden_dim)\n",
    "        db_h = grad_h_raw  # (hidden_dim,)\n",
    "\n",
    "        grad_h_prev = grad_h_raw @ self.W_hh.T  # (hidden_dim,)\n",
    "\n",
    "        return {\n",
    "            \"dW_hy\": dW_hy,\n",
    "            \"db_y\": db_y,\n",
    "            \"dW_xh\": dW_xh,\n",
    "            \"dW_hh\": dW_hh,\n",
    "            \"db_h\": db_h,\n",
    "            \"grad_h_prev\": grad_h_prev\n",
    "        }\n",
    "\n",
    "    def backward_sequence(self, x_seq, y_seq, pred_seq, debug=False):\n",
    "        \"\"\"\n",
    "        x_seq: (seq_len, input_dim)\n",
    "        y_seq: (seq_len, output_dim)\n",
    "        pred_seq: (seq_len, output_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = len(x_seq)\n",
    "\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "\n",
    "        grad_h_prev = np.zeros((self.hidden_dim,))\n",
    "\n",
    "        for t in reversed(range(seq_len)):\n",
    "            x_t = x_seq[t]\n",
    "            y_t_true = y_seq[t]\n",
    "            y_t_pred = pred_seq[t]\n",
    "            h_t = self.states['hidden_states'][t]\n",
    "            h_prev = self.states['hidden_states'][t - 1] if t > 0 else self.h_t\n",
    "\n",
    "            grad_output = self.grad(y_t_true, y_t_pred)\n",
    "            grad_output_total = grad_output \n",
    "\n",
    "            grads = self.backward_step(\n",
    "                grad_output=grad_output_total,\n",
    "                x_t=x_t,\n",
    "                h_t=h_t,\n",
    "                h_prev=h_prev\n",
    "            )\n",
    "            if debug:\n",
    "                print(f\"\\n=== Backward step t={t} ===\")\n",
    "                print(f\"grad_output: {grad_output}\")\n",
    "                print(f\"dW_xh norm: {np.linalg.norm(grads['dW_xh'])}\")\n",
    "                print(f\"dW_hh norm: {np.linalg.norm(grads['dW_hh'])}\")\n",
    "                print(f\"dW_hy norm: {np.linalg.norm(grads['dW_hy'])}\")\n",
    "                print(f\"db_h norm: {np.linalg.norm(grads['db_h'])}\")\n",
    "                print(f\"db_y norm: {np.linalg.norm(grads['db_y'])}\")\n",
    "                print(f\"grad_h_prev norm: {np.linalg.norm(grads['grad_h_prev'])}\")\n",
    "\n",
    "            dW_xh += grads[\"dW_xh\"]\n",
    "            dW_hh += grads[\"dW_hh\"]\n",
    "            db_h += grads[\"db_h\"]\n",
    "            dW_hy += grads[\"dW_hy\"]\n",
    "            db_y += grads[\"db_y\"]\n",
    "\n",
    "            grad_h_prev = grads[\"grad_h_prev\"]\n",
    "\n",
    "        return {\n",
    "            \"dW_xh\": dW_xh,\n",
    "            \"dW_hh\": dW_hh,\n",
    "            \"db_h\": db_h,\n",
    "            \"dW_hy\": dW_hy,\n",
    "            \"db_y\": db_y\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67414925, 0.94640503, 0.62870795, 0.96582873, 0.32993353,\n",
       "       0.22289821, 0.94950267, 0.48081432, 0.83340873, 0.09800429])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
