{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getPositionEncoding(seq_len, d, n=10000):\n",
    "    P = np.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            P[k, 2*i] = np.sin(k/denominator)\n",
    "            P[k, 2*i+1] = np.cos(k/denominator)\n",
    "    return P\n",
    "\n",
    "P = getPositionEncoding(seq_len=4, d=4, n=100)\n",
    "print(P)\n",
    "\n",
    "def plotSinusoid(k, d=512, n=10000):\n",
    "    x = np.arange(0, 100, 2)\n",
    "    denominator = np.power(n, 2*x/d)\n",
    "    y = np.sin(k/denominator)\n",
    "    plt.plot(x, y)\n",
    "    plt.title('k = ' + str(k))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))    \n",
    "for i in range(4):\n",
    "    plt.subplot(141 + i)\n",
    "    plotSinusoid(i*4)\n",
    "\n",
    "P = getPositionEncoding(seq_len=500, d=1024, n=10000)\n",
    "cax = plt.matshow(P)\n",
    "plt.gcf().colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, dropout_prob=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \"\"\"\n",
    "        pe(pos, 2i) = sin(pos / 10000 ^ (2i / d_model))\n",
    "        pe(pos, 2i + 1) = cos(pos / 10000 ^ (2i / d_model))\n",
    "        \"\"\"\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        P = torch.zeros((seq_len, d_model))  # Shape: (seq_len, d_model)\n",
    "\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1)  # Shape: (seq_len, 1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)   # Shape: (d_model / 2)\n",
    "        )\n",
    "        \n",
    "        P[:, 0::2] = torch.sin(position * div_term)  # Shape: (seq_len, d_model)\n",
    "        P[:, 1::2] = torch.cos(position * div_term)  # Shape: (seq_len, d_model)\n",
    "\n",
    "        P = P.unsqueeze(0)  # Shape: (1, seq_len, d_model)\n",
    "        self.register_buffer(\"pe\", P)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape: x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]  # Shape: (batch_size, seq_len, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_dim, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):  \n",
    "        # Shape: x: (batch_size, seq_len) - input token indices\n",
    "        return self.emb(x) * math.sqrt(self.d_model)  # Shape: (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_dim, d_model, seq_len, dropout_prob=0.1):\n",
    "        super(EmbeddingWithPositionalEncoding, self).__init__()\n",
    "\n",
    "        self.embedding = Embedding(vocab_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(seq_len, d_model, dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape: x: (batch_size, seq_len)\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, d_model)\n",
    "        x = self.positional_encoding(x)  # Shape: (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 20])\n",
      "Output shape: torch.Size([2, 20, 10])\n"
     ]
    }
   ],
   "source": [
    "vocab_dim = 50      # Vocabulary size (number of unique tokens)\n",
    "d_model = 10        # Embedding dimension\n",
    "seq_len = 30        # Maximum sequence length (for positional encoding)\n",
    "dropout_prob = 0.1  # Dropout probability\n",
    "batch_size = 2      # Batch size (number of sequences in a batch)\n",
    "input_seq_len = 20  # Sequence length for input (actual input length)\n",
    "\n",
    "# Initialize the combined model\n",
    "model = EmbeddingWithPositionalEncoding(vocab_dim, d_model, seq_len, dropout_prob)\n",
    "\n",
    "# Example 2D input vector: Shape (batch_size, input_seq_len)\n",
    "input_tensor = torch.randint(0, vocab_dim, (batch_size, input_seq_len))  # Shape: (2, 20)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "print(f\"Input shape: {input_tensor.shape}\")  # (2, 20)\n",
    "print(f\"Output shape: {output.shape}\")       # (2, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "# clones(nn.Linear(32, 32), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Shapes: \n",
    "    query - (batch_size, seq_len_q, d_k)\n",
    "    key - (batch_size, seq_len_kv, d_k)\n",
    "    value - (batch_size, seq_len_kv, d_v)\n",
    "    mask - (batch_size, seq_ken_q, seq_len_q)\n",
    "    \"\"\"\n",
    "    d_k = math.sqrt(query.size(-1))\n",
    "\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / d_k\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e-9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    \n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "# query = torch.rand(32, 128, 64)\n",
    "# key = torch.rand(32, 128, 64)\n",
    "# value = torch.rand(32, 128, 64)\n",
    "# dot_product_attention(query, key, value)[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"Error, d_model should be divisible to num_heads\"\n",
    "\n",
    "        self.dim_head = d_model // num_heads\n",
    "        self.h = num_heads\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4) # Q, K, V, Output linear\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attention = None\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        n_batches = query.size(0)\n",
    "\n",
    "        query, key, value = [\n",
    "            lin(x).view(n_batches, -1, self.h, self.dim_head).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        x, self.attention = dot_product_attention(\n",
    "            query, key, value, mask, self.dropout\n",
    "        )\n",
    "\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(n_batches, -1, self.h * self.dim_head)\n",
    "        )\n",
    "\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        gc.collect()\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multihead = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "\n",
    "# q = torch.rand((2, 100, 512))\n",
    "# k = torch.rand((2, 200, 512))\n",
    "# v = torch.rand((2, 200, 512))\n",
    "\n",
    "# multihead.forward(q, k, v).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "        self.feed_forward = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        feed_x = self.relu(self.feed_forward(x))\n",
    "        # norm_x = self.norm(feed_x)\n",
    "        return x + feed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, attention: MultiHeadAttention, feed_forward: PointWiseFeedForward, dropout: float = 0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.attention = attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.tensor, mask=None):\n",
    "        z = self.norm(x)\n",
    "        attention_out = self.dropout(self.attention(query=z, key=z, value=z, mask=mask)) + x\n",
    "        feed_forward_out = self.feed_forward(attention_out)\n",
    "\n",
    "        return self.norm(feed_forward_out + attention_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 64])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, num_heads, dropout_prob, vocab_dim, num_blocks=2):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.pos_embedding = EmbeddingWithPositionalEncoding(vocab_dim, d_model, seq_len, dropout_prob)\n",
    "\n",
    "        self.blocks = clones(\n",
    "            EncoderBlock(\n",
    "                d_model, \n",
    "                MultiHeadAttention(d_model, num_heads, dropout_prob), \n",
    "                PointWiseFeedForward(d_model), dropout_prob\n",
    "            ), \n",
    "            num_blocks\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = self.pos_embedding(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "vocab_dim = 50      # Vocabulary size (number of unique tokens)\n",
    "d_model = 64        # Embedding dimension\n",
    "seq_len = 30        # Maximum sequence length (for positional encoding)\n",
    "dropout_prob = 0.1  # Dropout probability\n",
    "batch_size = 2      # Batch size (number of sequences in a batch)\n",
    "input_seq_len = 20\n",
    "num_heads = 4\n",
    "\n",
    "enc = Encoder(d_model, seq_len, num_heads, dropout_prob, vocab_dim)\n",
    "\n",
    "enc.forward(torch.randint(0, vocab_dim, (batch_size, seq_len))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for masked multihead attention\n",
    "def decoder_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout_prob, attention: MultiHeadAttention, feed_forward: PointWiseFeedForward, *args, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.attention = attention\n",
    "        self.enc_dec_attention = attention\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.tensor, in_memory: torch.tensor, enc_mask, dec_mask):\n",
    "        z = self.norm1(x)\n",
    "        attention_out = self.dropout(self.attention(query=z, key=z, value=z, mask=dec_mask)) + x\n",
    "\n",
    "        z = self.norm2(attention_out)\n",
    "        enc_dec_attention = self.dropout(self.enc_dec_attention(query=attention_out, key=in_memory, value=in_memory, mask=dec_mask)) + attention_out\n",
    "\n",
    "        z = self.norm3(enc_dec_attention)\n",
    "        feed_forward_out = self.feed_forward(z)\n",
    "\n",
    "        return self.dropout(feed_forward_out) + enc_dec_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, num_heads, dropout_prob, vocab_dim, num_blocks=2, *args, **kwargs):\n",
    "        super(Decoder, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.pos_embedding = EmbeddingWithPositionalEncoding(vocab_dim, d_model, seq_len, dropout_prob)\n",
    "\n",
    "        self.blocks = clones(\n",
    "            DecoderBlock(\n",
    "                d_model, \n",
    "                num_heads=2,\n",
    "                dropout_prob=0.1,\n",
    "                attention=MultiHeadAttention(d_model, num_heads, dropout_prob), \n",
    "                feed_forward=PointWiseFeedForward(d_model)\n",
    "            ), \n",
    "            num_blocks\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x, memory, enc_mask, dec_mask):\n",
    "        out = self.pos_embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(out, memory, enc_mask, dec_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30, 64])\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "vocab_dim = 50\n",
    "d_model = 64\n",
    "seq_len = 30\n",
    "dropout_prob = 0.1\n",
    "batch_size = 2\n",
    "num_heads = 4\n",
    "num_blocks = 2\n",
    "\n",
    "# Initialize decoder\n",
    "decoder = Decoder(d_model, seq_len, num_heads, dropout_prob, vocab_dim, num_blocks)\n",
    "\n",
    "# Create dummy inputs\n",
    "x = torch.randint(0, vocab_dim, (batch_size, seq_len))  # Input tokens\n",
    "memory = torch.rand(batch_size, seq_len, d_model)      # Encoder output\n",
    "enc_mask = torch.ones(batch_size, seq_len, seq_len)    # Encoder mask (all ones for simplicity)\n",
    "dec_mask = decoder_mask(seq_len)                       # Decoder mask\n",
    "\n",
    "# Forward pass\n",
    "output = decoder(x, memory, enc_mask, dec_mask)\n",
    "print(output.size())  # Expected: (batch_size, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_dim, d_model, *args, **kwargs):\n",
    "        super(Generator, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.projection = nn.Linear(d_model, vocab_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, vocab_dim, seq_len, num_encoder_layers, num_decoder_layers, dropout_prob, *args, **kwargs):\n",
    "        super(Transformer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            d_model=d_model,\n",
    "            seq_len=seq_len,\n",
    "            num_heads=num_heads,\n",
    "            dropout_prob=dropout_prob,\n",
    "            vocab_dim=vocab_dim,\n",
    "            num_blocks=num_encoder_layers\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            d_model=d_model,\n",
    "            seq_len=seq_len,\n",
    "            num_heads=num_heads,\n",
    "            dropout_prob=dropout_prob,\n",
    "            vocab_dim=vocab_dim,\n",
    "            num_blocks=num_decoder_layers\n",
    "        )\n",
    "\n",
    "        self.generator = Generator(\n",
    "            vocab_dim=vocab_dim,\n",
    "            d_model=d_model\n",
    "        )\n",
    "\n",
    "    def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
    "        memory = self.encoder(enc_input)\n",
    "        print(memory.size())\n",
    "\n",
    "        decoder_out = self.decoder(dec_input, memory, enc_mask, dec_mask)\n",
    "        print(decoder_out.size())\n",
    "\n",
    "        out = self.generator(decoder_out)\n",
    "        print(out.size())\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30, 64])\n",
      "torch.Size([2, 30, 64])\n",
      "torch.Size([2, 30, 50])\n",
      "Output shape: torch.Size([2, 30, 50])\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "vocab_dim = 50      # Vocabulary size (number of unique tokens)\n",
    "d_model = 64        # Embedding dimension\n",
    "seq_len = 30        # Maximum sequence length\n",
    "dropout_prob = 0.1  # Dropout probability\n",
    "batch_size = 2      # Batch size\n",
    "num_heads = 4       # Number of attention heads\n",
    "num_encoder_blocks = 2  # Number of encoder layers\n",
    "num_decoder_blocks = 2  # Number of decoder layers\n",
    "\n",
    "# Initialize Transformer model\n",
    "transformer = Transformer(\n",
    "    vocab_dim=vocab_dim,\n",
    "    d_model=d_model,\n",
    "    seq_len=seq_len,\n",
    "    num_heads=num_heads,\n",
    "    dropout_prob=dropout_prob,\n",
    "    num_encoder_layers=num_encoder_blocks,\n",
    "    num_decoder_layers=num_decoder_blocks,\n",
    ")\n",
    "\n",
    "# Create dummy data\n",
    "src = torch.randint(0, vocab_dim, (batch_size, seq_len))  # Source input\n",
    "tgt = torch.randint(0, vocab_dim, (batch_size, seq_len))  # Target input\n",
    "src_mask = torch.ones(batch_size, seq_len, seq_len)      # Encoder mask\n",
    "tgt_mask = decoder_mask(seq_len)                         # Decoder mask\n",
    "\n",
    "# Forward pass\n",
    "output = transformer(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "# Output logits\n",
    "print(\"Output shape:\", output.size())  # Expected: (batch_size, seq_len, vocab_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multihead_attention():\n",
    "    batch_size, seq_len, d_model, num_heads = 2, 30, 64, 4\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    query = key = value = torch.rand(batch_size, seq_len, d_model)\n",
    "    mask = torch.ones(batch_size, seq_len, seq_len)\n",
    "    out = mha(query, key, value, mask)\n",
    "    assert out.size() == (batch_size, seq_len, d_model), \"Output shape mismatch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to make one forward pass\n",
    "\n",
    "# Create dummy data\n",
    "src = torch.randint(0, vocab_dim, (batch_size, seq_len))  # Source input\n",
    "tgt = torch.randint(0, vocab_dim, (batch_size, seq_len))  # Target input\n",
    "src_mask = torch.ones(batch_size, seq_len, seq_len)      # Encoder mask\n",
    "tgt_mask = decoder_mask(seq_len)                         # Decoder mask\n",
    "\n",
    "# Forward pass\n",
    "transformer = Transformer(\n",
    "    vocab_dim=vocab_dim,\n",
    "    d_model=d_model,\n",
    "    seq_len=seq_len,\n",
    "    num_heads=num_heads,\n",
    "    dropout_prob=dropout_prob,\n",
    "    num_encoder_layers=num_encoder_blocks,\n",
    "    num_decoder_layers=num_decoder_blocks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_mask(size):\n",
    "    # Generates a causal mask for the decoder\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.bool)\n",
    "    return ~mask  # Invert: True = allowed, False = blocked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (29) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[204], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m decoder_mask(seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Reshape output and target for loss calculation\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_dim), tgt_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[199], line 32\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, enc_input, dec_input, enc_mask, dec_mask)\u001b[0m\n\u001b[0;32m     29\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(enc_input)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(memory\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m---> 32\u001b[0m decoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoder_out\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m     35\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(decoder_out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[196], line 24\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, memory, enc_mask, dec_mask)\u001b[0m\n\u001b[0;32m     22\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(x)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[195], line 16\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, x, in_memory, enc_mask, dec_mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mtensor, in_memory: torch\u001b[38;5;241m.\u001b[39mtensor, enc_mask, dec_mask):\n\u001b[0;32m     15\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m---> 16\u001b[0m     attention_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_mask\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m     18\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(attention_out)\n\u001b[0;32m     19\u001b[0m     enc_dec_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_dec_attention(query\u001b[38;5;241m=\u001b[39mattention_out, key\u001b[38;5;241m=\u001b[39min_memory, value\u001b[38;5;241m=\u001b[39min_memory, mask\u001b[38;5;241m=\u001b[39mdec_mask)) \u001b[38;5;241m+\u001b[39m attention_out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[189], line 26\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     19\u001b[0m n_batches \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m query, key, value \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     22\u001b[0m     lin(x)\u001b[38;5;241m.\u001b[39mview(n_batches, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_head)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lin, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears, (query, key, value))\n\u001b[0;32m     24\u001b[0m ]\n\u001b[1;32m---> 26\u001b[0m x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mdot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     31\u001b[0m     x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;241m.\u001b[39mview(n_batches, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_head)\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m query\n",
      "Cell \u001b[1;32mIn[188], line 14\u001b[0m, in \u001b[0;36mdot_product_attention\u001b[1;34m(query, key, value, mask, dropout)\u001b[0m\n\u001b[0;32m     11\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query, key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m d_k\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m p_attn \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (29) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
    "\n",
    "# Generate synthetic data (identity mapping)\n",
    "num_batches = 100\n",
    "for batch in range(num_batches):\n",
    "    src = torch.randint(0, vocab_dim, (batch_size, seq_len))  # Source input\n",
    "    tgt = src.clone()  # Target output (identity mapping)\n",
    "    tgt_input = tgt[:, :-1]  # Remove the last token for input\n",
    "    tgt_output = tgt[:, 1:]  # Remove the first token for target\n",
    "\n",
    "    # Masks\n",
    "    src_mask = torch.ones(batch_size, seq_len, seq_len)\n",
    "    tgt_mask = decoder_mask(seq_len - 1)\n",
    "\n",
    "    # Forward pass\n",
    "    output = transformer(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "    # Reshape output and target for loss calculation\n",
    "    loss = criterion(output.view(-1, vocab_dim), tgt_output.view(-1))\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Batch {batch+1}/{num_batches}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
