{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f9735b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.nvtx as nvtx\n",
    "from einops import rearrange, einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ca26734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from cs336_basics.cs336_basics.model import BasicsTransformerLM\n",
    "from cs336_basics.cs336_basics.optimizer import get_cosine_lr, AdamW\n",
    "from cs336_basics.cs336_basics.data import get_batch\n",
    "from cs336_basics.cs336_basics.nn_utils import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff14d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0001)\n",
      "tensor(9.9531, dtype=torch.float16)\n",
      "tensor(10.0021)\n",
      "tensor(10.0021)\n"
     ]
    }
   ],
   "source": [
    "# s = torch.tensor(0,dtype=torch.float32)\n",
    "# for i in range(1000):\n",
    "#     s += torch.tensor(0.01,dtype=torch.float32)\n",
    "# print(s)\n",
    "# s = torch.tensor(0,dtype=torch.float16)\n",
    "# for i in range(1000):\n",
    "#     s += torch.tensor(0.01,dtype=torch.float16)\n",
    "# print(s)\n",
    "# s = torch.tensor(0,dtype=torch.float32)\n",
    "# for i in range(1000):\n",
    "#     s += torch.tensor(0.01,dtype=torch.float16)\n",
    "# print(s)\n",
    "# s = torch.tensor(0,dtype=torch.float32)\n",
    "# for i in range(1000):\n",
    "#     x = torch.tensor(0.01,dtype=torch.float16)\n",
    "#     s += x.type(torch.float32)\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8a7ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        print(f\"fc1: {x.dtype}\")\n",
    "        x = self.ln(x)\n",
    "        print(f\"ln: {x.dtype}\")\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fdd8c637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight grad dtype: torch.float32\n",
      "ln.weight grad dtype: torch.float32\n",
      "ln.bias grad dtype: torch.float32\n",
      "fc2.weight grad dtype: torch.float32\n",
      "\n",
      "\n",
      "fc1: torch.bfloat16\n",
      "ln: torch.float32\n",
      "logits type: torch.bfloat16\n",
      "loss type: torch.float32\n",
      "loss type after grad_scale: torch.float32\n",
      "\n",
      "\n",
      "fc1.weight grad dtype: torch.float32\n",
      "ln.weight grad dtype: torch.float32\n",
      "ln.bias grad dtype: torch.float32\n",
      "fc2.weight grad dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "model: nn.Module = ToyModel(in_features=10, out_features=10)\n",
    "dtype: torch.dtype = torch.float32\n",
    "device: str = \"cuda\"\n",
    "x: torch.Tensor = torch.rand((10, 10))\n",
    "target: torch.Tensor = torch.randint(0, 2, (10,))\n",
    "\n",
    "criterion = cross_entropy\n",
    "optimizer = AdamW(model.parameters())\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "model = model.to(device)\n",
    "x = x.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} grad dtype: {param.dtype}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "with torch.autocast(dtype=torch.bfloat16, device_type=device):\n",
    "    logits = model(x)\n",
    "    print(f\"logits type: {logits.dtype}\")\n",
    "    loss = criterion(logits, target)\n",
    "    print(f\"loss type: {loss.dtype}\")\n",
    "scaler.scale(loss).backward()\n",
    "print(f\"loss type after grad_scale: {loss.dtype}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad dtype: {param.grad.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a626f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float, Bool, Int\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from cs336_basics.cs336_basics.nn_utils import *\n",
    "from cs336_basics.cs336_basics.optimizer import AdamW\n",
    "from cs336_basics.cs336_basics.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f007f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalMultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention\n",
    "\n",
    "    This function implements section 3.2.2 of the Transformer paper. In particular,\n",
    "    given an input tensor of shape `(batch_size, sequence_length, d_model)`, we project\n",
    "    it to create queries, keys, and values, and then perform causal multi-headed attention with\n",
    "    those queries, keys, and values.\n",
    "\n",
    "    Args:\n",
    "        d_model: int\n",
    "            The dimensionality of the model embeddings and sublayer outputs.\n",
    "        num_heads: int\n",
    "            Number of heads to use in multi-headed attention. `d_model` must be\n",
    "            evenly divisible by `num_heads`.\n",
    "        positional_encoder: RotaryEmbedding\n",
    "            The RoPE module to use.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape `(batch_size, sequence_length, d_model)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.output_proj = Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V) -> Float[Tensor, \" ... seq d_v\"]:\n",
    "\n",
    "        # Take apart each head from the embedding dimension of Q, K, V to shape (..., num_heads, seq_len, d_k).\n",
    "        # Q, K, V = (\n",
    "        #     rearrange(X, \"... seq (heads d) -> ... heads seq d\", heads=self.num_heads)\n",
    "        #     for X in (Q, K, V)\n",
    "        # )  # fmt: skip\n",
    "\n",
    "        # if token_positions is None:\n",
    "        #     token_positions = einx.rearrange(\"seq -> b... seq\", torch.arange(sequence_length, device=x.device), b=[1] * len(b))\n",
    "\n",
    "        # # Duplicate token positions for each head\n",
    "        # token_positions = rearrange(token_positions, \"... seq -> ... 1 seq\")\n",
    "\n",
    "        # Construct causal mask\n",
    "\n",
    "        # Shape: (..., num_heads, sequence_length, d_k)\n",
    "        attn_output = scaled_dot_product_attention(K=K, Q=Q, V=V)\n",
    "\n",
    "        # Concatenate the attention output from all heads.\n",
    "        # (..., sequence_length, num_heads * d_v).\n",
    "        # Apply the output projection\n",
    "        output = self.output_proj(attn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e869336",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CausalMultiHeadSelfAttention(d_model=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6911e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ccd9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a1837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbad7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab8917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9982fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asign_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
