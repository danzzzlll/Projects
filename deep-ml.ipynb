{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is some exercises from [deep-ml](https://www.deep-ml.com/problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"it is sunny morning, when i want to go to school\",\n",
    "    \"when it is dead you cant create something new что\",\n",
    "    \"что такое осень, это небо, небо и слякоть под ногами it has been go\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def tf(doc, uniq_words):\n",
    "    \"\"\"Compute term frequency for a single document\"\"\"\n",
    "    word_count = Counter(doc.split())\n",
    "    total_words = len(doc.split())  \n",
    "    return np.array([word_count[word] / total_words if total_words > 0 else 0 for word in uniq_words])\n",
    "\n",
    "def idf(docs, uniq_words):\n",
    "    \"\"\"Compute inverse document frequency across all documents\"\"\"\n",
    "    n = len(docs)\n",
    "    df = np.array([sum(1 for doc in docs if word in doc.split()) for word in uniq_words], dtype=np.float64)\n",
    "    idf_m = np.log(n / (df + 1e-6))  \n",
    "    return idf_m\n",
    "\n",
    "def tfidf(docs, word=None):\n",
    "    \"\"\"Compute the TF-IDF matrix for all documents\"\"\"\n",
    "    uniq_words = list(set(word for doc in docs for word in doc.split()))\n",
    "    idf_values = idf(docs, uniq_words)\n",
    "    tfidf_matrix = np.array([tf(doc, uniq_words) * idf_values for doc in docs])\n",
    "    return tfidf_matrix, uniq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00000000e+00,  3.68604189e-02,  9.98737535e-02,\n",
       "          1.99747507e-01,  0.00000000e+00, -3.03030253e-08,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          9.98737535e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  9.98737535e-02,  0.00000000e+00,\n",
       "          3.68604189e-02,  9.98737535e-02,  0.00000000e+00,\n",
       "          9.98737535e-02,  0.00000000e+00,  3.68604189e-02,\n",
       "          0.00000000e+00],\n",
       "        [ 1.09861129e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.09861129e-01, -3.33333278e-08,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  4.05464608e-02,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.09861129e-01,  1.09861129e-01,\n",
       "          0.00000000e+00,  1.09861129e-01,  1.09861129e-01,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          4.05464608e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  4.05464608e-02,\n",
       "          0.00000000e+00],\n",
       "        [ 0.00000000e+00,  2.89617577e-02,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00, -2.38095198e-08,\n",
       "          7.84722349e-02,  7.84722349e-02,  7.84722349e-02,\n",
       "          0.00000000e+00,  2.89617577e-02,  7.84722349e-02,\n",
       "          7.84722349e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "          7.84722349e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "          7.84722349e-02,  0.00000000e+00,  7.84722349e-02,\n",
       "          0.00000000e+00,  0.00000000e+00,  7.84722349e-02,\n",
       "          0.00000000e+00,  7.84722349e-02,  0.00000000e+00,\n",
       "          7.84722349e-02]]),\n",
       " ['new',\n",
       "  'go',\n",
       "  'i',\n",
       "  'to',\n",
       "  'something',\n",
       "  'it',\n",
       "  'небо,',\n",
       "  'осень,',\n",
       "  'и',\n",
       "  'want',\n",
       "  'что',\n",
       "  'has',\n",
       "  'это',\n",
       "  'cant',\n",
       "  'create',\n",
       "  'под',\n",
       "  'dead',\n",
       "  'you',\n",
       "  'been',\n",
       "  'school',\n",
       "  'небо',\n",
       "  'when',\n",
       "  'morning,',\n",
       "  'ногами',\n",
       "  'sunny',\n",
       "  'такое',\n",
       "  'is',\n",
       "  'слякоть'])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def tf(doc, uniq_words):\n",
    "    \"\"\"Compute term frequency for a single document\"\"\"\n",
    "    word_count = Counter(doc)\n",
    "    total_words = len(doc)  \n",
    "    return np.array([word_count[word] / total_words if total_words > 0 else 0 for word in uniq_words])\n",
    "\n",
    "def idf(docs, uniq_words):\n",
    "    \"\"\"Compute inverse document frequency across all documents\"\"\"\n",
    "    n = len(docs)\n",
    "    df = np.array([sum(1 for doc in docs if word in doc) for word in uniq_words], dtype=np.float64)\n",
    "    idf_m = np.log(n / (df + 1))  \n",
    "    return idf_m\n",
    "\n",
    "def compute_tf_idf(corpus, query):\n",
    "\t\"\"\"\n",
    "\tCompute TF-IDF scores for a query against a corpus of documents.\n",
    "    \n",
    "\t:param corpus: List of documents, where each document is a list of words\n",
    "\t:param query: List of words in the query\n",
    "\t:return: List of lists containing TF-IDF scores for the query words in each document\n",
    "\t\"\"\"\n",
    "\tuniq_words = list(set(word for doc in corpus for word in doc))\n",
    "\tidf_values = idf(corpus, uniq_words)\n",
    "\ttfidf_matrix = np.array([tf(doc, uniq_words) * idf_values for doc in corpus])\n",
    "\tprint(uniq_words)\n",
    "\tindexes = [uniq_words.index(q) for q in query]\n",
    "\tprint(indexes)\n",
    "\treturn tfidf_matrix, [[elem] for elem in tfidf_matrix[:, indexes]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['over', 'dog', 'bird', 'mat', 'flew', 'chased', 'sat', 'on', 'the', 'cat']\n",
      "[9]\n",
      "(array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.06757752,  0.06757752, -0.09589402,  0.        ],\n",
      "       [ 0.        ,  0.08109302,  0.        ,  0.        ,  0.        ,\n",
      "         0.08109302,  0.        ,  0.        , -0.11507283,  0.        ],\n",
      "       [ 0.06757752,  0.        ,  0.06757752,  0.        ,  0.06757752,\n",
      "         0.        ,  0.        ,  0.        , -0.09589402,  0.        ]]), [[array([0.])], [array([0.])], [array([0.])]])\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n",
    "    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n",
    "]\n",
    "query = [\"cat\"]\n",
    "\n",
    "print(compute_tf_idf(corpus, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix-Vector Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2], [2, 4]]\n",
    "b = [1, 2]\n",
    "\n",
    "def matrix_dot_vector(a, b):\n",
    "    if len(a[0]) != len(b):\n",
    "        return -1\n",
    "    return [sum(row[i] * b[i] for i in range(len(b))) for row in a]\n",
    "\n",
    "def matrix_dot_vector(a, b):\n",
    "    # Return a list where each element is the dot product of a row of 'a' with 'b'.\n",
    "    # If the number of columns in 'a' does not match the length of 'b', return -1.\n",
    "    if len(a[0]) != len(b):\n",
    "        return -1\n",
    "    answer = []\n",
    "    s = 0\n",
    "    for row in a:\n",
    "        for ind, elem in enumerate(row):\n",
    "            s += elem * b[ind]\n",
    "        answer.append(s)\n",
    "        s = 0\n",
    "        \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6], [7, 8]]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape Matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = [[1,2,3,4],[5,6,7,8]]\n",
    "new_shape = (4, 2)\n",
    "\n",
    "def reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n",
    "    #Write your code here and return a python list after reshaping by using numpy's tolist() method \n",
    "    try:\n",
    "        return np.array(a).reshape(new_shape).tolist()\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "reshape_matrix(a, new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean by Row or Column\n",
    "\n",
    "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "mode = 'column'\n",
    "\n",
    "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n",
    "    if mode == 'row':\n",
    "        return [\n",
    "            sum(row) / len(row) for row in matrix\n",
    "        ]\n",
    "    # s = 0\n",
    "    # answer = []\n",
    "    # for ind, elem in enumerate(matrix):\n",
    "    #     for row in matrix:\n",
    "    #         s += row[ind]\n",
    "    #     answer.append(s / len(matrix))\n",
    "    #     s = 0\n",
    "    # return answer\n",
    "    return [sum(row[i] for row in matrix) / len(matrix) for i in range(len(matrix[0]))]\n",
    "\n",
    "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "calculate_matrix_mean(matrix, 'column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4], [6, 8]]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar Multiplication of a Matrix\n",
    "\n",
    "matrix = [[1, 2, 3], [3, 4]]\n",
    "scalar = 2\n",
    "\n",
    "def scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n",
    "\n",
    "\treturn\t[\n",
    "\t\t\t[matrix[j][i] * scalar for i in range(len(matrix))]\n",
    "\t\t\tfor j in range(len(matrix))\n",
    "        ]\n",
    "\n",
    "scalar_multiply(matrix, scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Eigenvalues of a Matrix\n",
    "\n",
    "matrix = [[2, 1], [1, 2]]\n",
    "import math\n",
    "def calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n",
    "    a = matrix[0][0]\n",
    "    b = matrix[0][1]\n",
    "    c = matrix[1][0]\n",
    "    d = matrix[1][1]\n",
    "\n",
    "    tr = a + d\n",
    "    det = a * d - b * c\n",
    "\n",
    "    descrim = tr * tr - 4 * 1 * det\n",
    "\n",
    "    x1 = (tr + math.sqrt(descrim)) / 2 * 1\n",
    "    x2 = (tr - math.sqrt(descrim)) / 2 * 1\n",
    "\n",
    "    return sorted([x1, x2], reverse=True)\n",
    "\n",
    "calculate_eigenvalues(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Using Normal Equation\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([1, 2, 3])\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    y = y.reshape(-1, 1)\n",
    "    for _ in range(iterations):\n",
    "        y_pred = X @ theta\n",
    "        error = y_pred - y\n",
    "        theta -= grad(m, X, error) * alpha\n",
    "\n",
    "    return np.round(theta, 4)\n",
    "\n",
    "def grad(m, X, error):\n",
    "    return (1 / m) * (X.T @ error)\n",
    "\n",
    "linear_regression_gradient_descent(X, y, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Implement TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n",
    "    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n",
    "]\n",
    "query = [\"cat\"]\n",
    "\n",
    "def compute_tf(doc, uniq_words):\n",
    "    \"\"\"Compute term frequency for a document\"\"\"\n",
    "    word_count = {word: 0 for word in uniq_words}\n",
    "    total_words = len(doc)\n",
    "\n",
    "    for word in doc:\n",
    "        word_count[word] += 1\n",
    "    \n",
    "    return {word: word_count[word] / total_words if total_words > 0 else 0 for word in uniq_words}\n",
    "\n",
    "\n",
    "def compute_idf(corpus, uniq_words):\n",
    "    \"\"\"Compute inverse document frequency for all words in the corpus\"\"\"\n",
    "    num_docs = len(corpus)\n",
    "    doc_freq = {word: 0 for word in uniq_words}\n",
    "\n",
    "    for word in uniq_words:\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                doc_freq[word] += 1\n",
    "    \n",
    "    return {word: math.log((num_docs + 1) / (doc_freq[word] + 1)) + 1 for word in uniq_words}\n",
    "\t\n",
    "\n",
    "def compute_tf_idf(corpus, query):\n",
    "\t\"\"\"\n",
    "\tCompute TF-IDF scores for a query against a corpus of documents.\n",
    "    \n",
    "\t:param corpus: List of documents, where each document is a list of words\n",
    "\t:param query: List of words in the query\n",
    "\t:return: List of lists containing TF-IDF scores for the query words in each document\n",
    "\t\"\"\"\n",
    "\tif not corpus:\n",
    "\t\treturn []\n",
    "\tuniq_words = list(\n",
    "\t\tset(\n",
    "\t\t\t[\n",
    "\t\t\t\tword for doc in corpus\n",
    "\t\t\t\tfor word in doc\n",
    "\t\t\t]\n",
    "\t\t))\n",
    "      \n",
    "\tidf_values = compute_idf(corpus, uniq_words)\n",
    "\t\n",
    "\ttfidf_matrix = []\n",
    "\tfor doc in corpus:\n",
    "\t\ttf = compute_tf(doc=doc, uniq_words=uniq_words)\n",
    "\t\ttfidf_scores = [tf.get(q, 0) * idf_values.get(q, 0) for q in query]\n",
    "\t\ttfidf_matrix.append(tfidf_scores)\n",
    "\treturn np.round(tfidf_matrix, 5).tolist()\n",
    "\t\n",
    "compute_tf_idf(corpus, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Neuron\n",
    "\n",
    "features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]]\n",
    "labels = [0, 1, 0]\n",
    "weights = [0.7, -0.4]\n",
    "bias = -0.1\n",
    "\n",
    "import math\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "def single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n",
    "\n",
    "    out_matrix = [sum(row[j] * weights[j] for j in range(len(weights))) for row in features]\n",
    "    bias_matrix = [elem + bias for elem in out_matrix]\n",
    "    probabilities = [round(sigmoid(elem), 4) for elem in bias_matrix]\n",
    "    mse = round(sum([math.pow((prob - true), 2) for prob, true in zip(probabilities,labels )]) / len(labels), 4)\n",
    "    return probabilities, mse\n",
    "\n",
    "single_neuron_model(features, labels, weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-0.489, 0.8924], -0.5059, [0.3033, 0.2942, 0.2856, 0.2774, 0.2697, 0.2625, 0.2556, 0.2493, 0.2433, 0.2377, 0.2324, 0.2274, 0.2228, 0.2184, 0.2142, 0.2103, 0.2065, 0.2029, 0.1995, 0.1963, 0.1932, 0.1902, 0.1873, 0.1845, 0.1818, 0.1792, 0.1767, 0.1742, 0.1718, 0.1695, 0.1672, 0.165, 0.1629, 0.1608, 0.1588, 0.1567, 0.1548, 0.1529, 0.151, 0.1492, 0.1474, 0.1456, 0.1439, 0.1422, 0.1405, 0.1389, 0.1373, 0.1357, 0.1342, 0.1326, 0.1312, 0.1297, 0.1283, 0.1269, 0.1255, 0.1241, 0.1228, 0.1215, 0.1202, 0.1189, 0.1177, 0.1165, 0.1153, 0.1141, 0.1129, 0.1118, 0.1107, 0.1096, 0.1085, 0.1074, 0.1064, 0.1054, 0.1043, 0.1034, 0.1024, 0.1014, 0.1005, 0.0995, 0.0986, 0.0977, 0.0968, 0.0959, 0.0951, 0.0942, 0.0934, 0.0925, 0.0917, 0.0909, 0.0901, 0.0894, 0.0886, 0.0878, 0.0871, 0.0864, 0.0856, 0.0849, 0.0842, 0.0835, 0.0828, 0.0822])\n"
     ]
    }
   ],
   "source": [
    "# single neron with backprop\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int):\n",
    "    weights = np.array(initial_weights, dtype=np.float64)\n",
    "    bias = float(initial_bias)\n",
    "    m = len(labels)  # Number of samples\n",
    "    mse_values = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Compute predictions\n",
    "        linear_output = np.dot(features, weights) + bias\n",
    "        predictions = sigmoid(linear_output)\n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        errors = predictions - labels\n",
    "        mse = np.mean(errors ** 2)\n",
    "        mse_values.append(round(mse, 4).item())\n",
    "\n",
    "        # Compute gradients\n",
    "        gradient_weights = (2 / m) * np.dot(features.T, errors * predictions * (1 - predictions))\n",
    "        gradient_bias = (2 / m) * np.sum(errors * predictions * (1 - predictions))\n",
    "\n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * gradient_weights\n",
    "        bias -= learning_rate * gradient_bias\n",
    "\n",
    "    return np.round(weights, 4).tolist(), round(bias, 4).item(), mse_values\n",
    "\n",
    "# Test Case\n",
    "print(train_neuron(\n",
    "    np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]),\n",
    "    np.array([1, 0, 0]),\n",
    "    np.array([0.1, -0.2]),\n",
    "    0.0,\n",
    "    0.1,\n",
    "    100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4076, -1.4076, -0.4076])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log softmax\n",
    "\n",
    "A = np.array([1, 2, 3])\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def log(scores):\n",
    "\treturn math.log(sum([\n",
    "\t\t\tmath.exp(x - max(scores)) for x in scores\n",
    "\t\t]))\n",
    "\n",
    "def log_softmax(scores: list) -> np.ndarray:\n",
    "\treturn np.array([\n",
    "\t\tround((x - max(scores) - log(scores)), 4)\n",
    "\t\tfor x in scores\n",
    "\t])\n",
    "\n",
    "\n",
    "log_softmax(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=2, grad=1) Value(data=5, grad=10) Value(data=10, grad=5) Value(data=52, grad=1) Value(data=52, grad=1)\n"
     ]
    }
   ],
   "source": [
    "# Implementing Basic Autograd Operations\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\t\t\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        out = Value(data=(self.data + other.data), _children=(self, other), _op=\"+\")\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        out = Value(data=(self.data * other.data), _children=(self, other), _op=\"*\")\n",
    "        def _backward():\n",
    "            self.grad += (other.data * out.grad)\n",
    "            other.grad += (self.data * out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(data=max(0, self.data), _children=(self,), _op=\"relu\")\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\t\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "a = Value(2)\n",
    "b = Value(5)\n",
    "c = Value(10)\n",
    "d = a + b * c\n",
    "e = d.relu()\n",
    "e.backward()\n",
    "print(a, b, c, d, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Pattern Weaver's Code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n = 5\n",
    "crystal_values = [4, 2, 7, 1, 9]\n",
    "dimension = 1\n",
    "\n",
    "def softmax(values):\n",
    "    exp_values = np.exp(values - np.max(values)) \n",
    "    return exp_values / np.sum(exp_values, axis=0)\n",
    "\n",
    "def pattern_weaver(n, crystal_values, dimension):\n",
    "    crystal_values = np.array(crystal_values).reshape(n, dimension)\n",
    "    attention_scores = crystal_values @ crystal_values.T \n",
    "    attention_weights = np.apply_along_axis(softmax, 1, attention_scores)\n",
    "    weighted_patterns = attention_weights @ crystal_values\n",
    "    return np.round(weighted_patterns.flatten(), 4).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: [ 0.19001678 12.29000026]\n"
     ]
    }
   ],
   "source": [
    "# Implement Adam Optimization Algorithm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n",
    "    m = 0\n",
    "    v = 0\n",
    "    for it in range(num_iterations):\n",
    "        it += 1\n",
    "        gradient = grad(x0)\n",
    "        \n",
    "        m = beta1 * m + (1 - beta1) * gradient\n",
    "        v = beta2 * v + (1 - beta2) * gradient * gradient\n",
    "\n",
    "        m_up = m / (1 - beta1 ** it)    \n",
    "        v_up = v / (1 - beta2 ** it)\n",
    "\n",
    "        x0 = x0 - learning_rate * m_up / (np.sqrt(v_up) + epsilon)\n",
    "        \n",
    "    return x0\n",
    "\n",
    "def objective_function(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient(x):\n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "\n",
    "x0 = np.array([0.2, 12.3])\n",
    "x_opt = adam_optimizer(objective_function, gradient, x0)\n",
    "\n",
    "print(\"Optimized parameters:\", x_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.999, 0.01, 1e-05)"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using the Adam optimizer.\n",
    "    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n",
    "    :param parameter: Current parameter value\n",
    "    :param grad: Current gradient\n",
    "    :param m: First moment estimate\n",
    "    :param v: Second moment estimate\n",
    "    :param t: Current timestep\n",
    "    :param learning_rate: Learning rate (default=0.001)\n",
    "    :param beta1: First moment decay rate (default=0.9)\n",
    "    :param beta2: Second moment decay rate (default=0.999)\n",
    "    :param epsilon: Small constant for numerical stability (default=1e-8)\n",
    "    :return: tuple: (updated_parameter, updated_m, updated_v)\n",
    "    \"\"\"\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * grad * grad\n",
    "\n",
    "    m_up = m / (1 - beta1 ** t)    \n",
    "    v_up = v / (1 - beta2 ** t)\n",
    "\n",
    "    x0 = learning_rate * m_up / (np.sqrt(v_up) + epsilon)\n",
    "    parameter = parameter - x0\n",
    "\n",
    "    # Your code here\n",
    "    return np.round(parameter,5), np.round(m,5), np.round(v,5)\n",
    "\n",
    "\n",
    "parameter = 1.0\n",
    "grad = 0.1\n",
    "m = 0.0\n",
    "v = 0.0\n",
    "t = 1\n",
    "\n",
    "adam_optimizer(parameter, grad, m, v, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output: [[ 0.10162127 -0.33551992 -0.64490545]]\n",
      "Backward pass output: [[ 0.20676524 -0.23208937]]\n"
     ]
    }
   ],
   "source": [
    "# Dense Layer\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class Layer(object):\n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape\n",
    "\n",
    "    def layer_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def parameters(self):\n",
    "        return 0\n",
    "\n",
    "    def forward_pass(self, X, training):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def output_shape(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, n_units, input_shape=None):\n",
    "        self.layer_input = None\n",
    "        self.n_units = n_units\n",
    "        self.trainable = True\n",
    "        self.W = None\n",
    "        self.w0 = None\n",
    "        self.input_shape = input_shape  # Optional\n",
    "\n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape  # Ensure input shape is set before initialization\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        if self.input_shape is None:\n",
    "            raise ValueError(\"Input shape must be set before initializing the layer.\")\n",
    "\n",
    "        limit = 1 / math.sqrt(self.input_shape[0])\n",
    "        self.W = np.random.uniform(low=-limit, high=limit, size=(self.input_shape[0], self.n_units))\n",
    "        self.w0 = np.zeros(shape=(1, self.n_units))\n",
    "\n",
    "        # Assign optimizer without copying\n",
    "        self.W_opt = optimizer\n",
    "        self.w0_opt = optimizer\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.input_shape[0] * self.n_units + self.n_units\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "        return X.dot(self.W) + self.w0 \n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        if self.trainable:\n",
    "            grad_W = self.layer_input.T.dot(accum_grad)\n",
    "            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n",
    "\n",
    "            self.W = self.W_opt.update(self.W, grad_W)\n",
    "            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n",
    "\n",
    "        return accum_grad.dot(self.W.T)\n",
    "\n",
    "class MockOptimizer:\n",
    "    def update(self, weights, grad):\n",
    "        return weights - 0.01 * grad    \n",
    "\n",
    "optimizer = MockOptimizer()\n",
    "\n",
    "dense_layer = Dense(n_units=3, input_shape=(2,))\n",
    "dense_layer.initialize(optimizer)\n",
    "\n",
    "X = np.array([[1, 2]])\n",
    "output = dense_layer.forward_pass(X)\n",
    "print(\"Forward pass output:\", output)\n",
    "\n",
    "accum_grad = np.array([[0.1, 0.2, 0.3]])\n",
    "back_output = dense_layer.backward_pass(accum_grad)\n",
    "print(\"Backward pass output:\", back_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention formula\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    return X.dot(W_q), X.dot(W_k), X.dot(W_v)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "\n",
    "    attn_score = Q.dot(K.T) / np.sqrt(K.shape[0])\n",
    "\n",
    "    sft = softmax(attn_score)\n",
    "\n",
    "    attn_output = sft.dot(V)\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PReLU\n",
    "\n",
    "def prelu(x: float, alpha: float = 0.25):\n",
    "    return (x if x > 0 else alpha * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def softplus(x: float):\n",
    "    return round(math.log(1 + math.exp(x)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product Calculator\n",
    "\n",
    "vec1 = np.array([1, 2, 3])\n",
    "vec2 = np.array([4, 5, 6])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_dot_product(vec1, vec2) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the dot product of two vectors.\n",
    "    Args:\n",
    "        vec1 (numpy.ndarray): 1D array representing the first vector.\n",
    "        vec2 (numpy.ndarray): 1D array representing the second vector.\n",
    "    \"\"\"\n",
    "    return np.sum(vec1 * vec2).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10, 18])"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Cosine Similarity Between Vectors\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.sum(v1 * v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Weights: [0.6429088 0.2228896]\n",
      "Iteration 100, Weights: [1.05468669 0.83144605]\n",
      "Iteration 200, Weights: [1.01575264 0.95208775]\n",
      "Iteration 300, Weights: [1.00468432 0.98641084]\n",
      "Iteration 400, Weights: [1.00131172 0.99612201]\n",
      "Iteration 500, Weights: [1.00036722 0.99889459]\n",
      "Iteration 600, Weights: [1.0001058  0.99968626]\n",
      "Iteration 700, Weights: [1.00003039 0.99991065]\n",
      "Iteration 800, Weights: [1.00000832 0.99997444]\n",
      "Iteration 900, Weights: [1.00000241 0.99999274]\n",
      "[1.00000068 0.99999791]\n",
      "Iteration 0, Weights: [0.63245056 0.23409664]\n",
      "Iteration 100, Weights: [1.05014356 0.83864699]\n",
      "Iteration 200, Weights: [1.01416661 0.95441439]\n",
      "Iteration 300, Weights: [1.00400236 0.98712111]\n",
      "Iteration 400, Weights: [1.00113075 0.99636144]\n",
      "Iteration 500, Weights: [1.00031946 0.99897203]\n",
      "Iteration 600, Weights: [1.00009025 0.99970958]\n",
      "Iteration 700, Weights: [1.0000255  0.99991795]\n",
      "Iteration 800, Weights: [1.0000072  0.99997682]\n",
      "Iteration 900, Weights: [1.00000204 0.99999345]\n",
      "[1.00000058 0.99999813]\n",
      "Iteration 0, Weights: [0.3813 0.1286]\n",
      "Iteration 100, Weights: [1.11009574 0.67625145]\n",
      "Iteration 200, Weights: [1.06003642 0.82418964]\n",
      "Iteration 300, Weights: [1.03238968 0.90429545]\n",
      "Iteration 400, Weights: [1.01756141 0.94799669]\n",
      "Iteration 500, Weights: [1.00954276 0.97173712]\n",
      "Iteration 600, Weights: [1.00521549 0.98462982]\n",
      "Iteration 700, Weights: [1.00278045 0.99162499]\n",
      "Iteration 800, Weights: [1.00153512 0.99545529]\n",
      "Iteration 900, Weights: [1.00082271 0.9975317 ]\n",
      "[1.00045749 0.99865309]\n"
     ]
    }
   ],
   "source": [
    "# Implement Gradient Descent Variants with MSE Loss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "batch_size = 2\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.zeros(X.shape[1])\n",
    "\n",
    "def grad(X, error):\n",
    "    return 2 / X.shape[0] * (X.T @ error)\n",
    "\n",
    "def grad_one(x, error):\n",
    "    return 2 * error * x\n",
    "\n",
    "def batchify(X, y, batch_size):\n",
    "    indices = np.random.permutation(len(y))\n",
    "    X, y = X[indices], y[indices]\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        yield X[i:i + batch_size], y[i:i + batch_size]\n",
    "\n",
    "def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n",
    "    for i in range(n_iterations):\n",
    "        if method == \"batch\":\n",
    "            y_pred = X.dot(weights)\n",
    "            error = y_pred - y\n",
    "            weights = weights - learning_rate * grad(X, error)\n",
    "        elif method == \"stochastic\":\n",
    "            for example, target in zip(X, y):\n",
    "                y_pred = example.dot(weights)\n",
    "                error = y_pred - target\n",
    "                weights = weights - learning_rate * grad_one(example, error)\n",
    "        elif method == \"mini_batch\":\n",
    "            generator = batchify(X, y, batch_size)\n",
    "            for x_, y_ in generator:\n",
    "                y_pred = x_.dot(weights)\n",
    "                error = y_pred - y_\n",
    "                weights = weights - learning_rate * grad(x_, error)\n",
    "        if i % 100 == 0: \n",
    "            print(f\"Iteration {i}, Weights: {weights}\")\n",
    "    return weights\n",
    "\n",
    "# Test Batch Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='mini_batch')\n",
    "print(final_weights)\n",
    "# Test Stochastic Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n",
    "print(final_weights)\n",
    "# # Test Mini-Batch Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n",
    "print(final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.42371644, 0.42371644]), np.float64(0.15385068459377865))"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement Lasso Regression using Gradient Descent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [1, 1], [2, 2]])\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "def l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        y_pred = X @ weights + bias\n",
    "        error = y_pred - y\n",
    "\n",
    "        grad_w = X.T.dot(error) / n_samples + alpha * np.sign(weights)\n",
    "        grad_b = np.sum(error) / n_samples\n",
    "\n",
    "        weights = weights - learning_rate * grad_w\n",
    "        bias = bias - learning_rate * grad_b\n",
    "\n",
    "        weights[np.abs(weights) < tol] = 0 \n",
    "\n",
    "        if np.linalg.norm(grad_w, ord=1) < tol:\n",
    "            break \n",
    "\n",
    "        # print(error, weights, bias)\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "weights, bias = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\n",
    "\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Implementing a Simple RNN\n",
    "Write a Python function that implements a simple Recurrent Neural Network (RNN) cell. The function should process a sequence of input vectors and produce the final hidden state. Use the tanh activation function for the hidden state updates. The function should take as inputs the sequence of input vectors, the initial hidden state, the weight matrices for input-to-hidden and hidden-to-hidden connections, and the bias vector. The function should return the final hidden state after processing the entire sequence, rounded to four decimal places.\n",
    "\"\"\"\n",
    "\n",
    "input_sequence = [[1.0], [2.0], [3.0]]\n",
    "initial_hidden_state = [0.0]\n",
    "Wx = [[0.5]]  # Input to hidden weights\n",
    "Wh = [[0.8]]  # Hidden to hidden weights\n",
    "b = [0.0]     # Bias\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n",
    "    input_sequence = np.array(input_sequence)\n",
    "    initial_hidden_state = np.array(initial_hidden_state)\n",
    "    h = initial_hidden_state.copy()\n",
    "    for x in input_sequence:\n",
    "        # print(Wx, x, Wx @ x, Wh, h, Wh @ h, b)\n",
    "        h = np.tanh(Wx @ x + Wh @ h + b)\n",
    "        # print(h)\n",
    "    final_hidden_state = h.copy()\n",
    "    return final_hidden_state\n",
    "\n",
    "rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.499972375871717\n",
      "12.5311058226694\n",
      "11.636038910405476\n",
      "10.804035865168329\n",
      "10.024107182272504\n",
      "9.28324192309151\n",
      "8.565409952880177\n",
      "7.8517285480392385\n",
      "7.123178156988229\n",
      "6.367053006719603\n",
      "5.585268740038903\n",
      "4.798051082724053\n",
      "4.037924653925035\n",
      "3.3380701127078347\n",
      "2.7232423617334596\n",
      "2.2061289825738197\n",
      "1.787722282984615\n",
      "1.4600841371681974\n",
      "1.2101267902294872\n",
      "1.0230919587754836\n",
      "0.88495019395496\n",
      "0.7836361839446303\n",
      "0.7094251145839994\n",
      "0.6548170977594554\n",
      "0.6142056009132068\n",
      "0.5834936644586821\n",
      "0.559738302610401\n",
      "0.5408535204106243\n",
      "0.5253766066853931\n",
      "0.5122910527647776\n",
      "0.5008957927861598\n",
      "0.49071042052507463\n",
      "0.481407438262769\n",
      "0.4727643655031737\n",
      "0.4646302069916315\n",
      "0.4569021806051656\n",
      "0.44950970831653614\n",
      "0.44240350854473165\n",
      "0.4355482451845629\n",
      "0.42891763701673025\n",
      "0.4224912536626769\n",
      "0.4162524546671914\n",
      "0.41018709237224527\n",
      "0.4042827158714206\n",
      "0.39852809608761497\n",
      "0.39291295054886355\n",
      "0.3874277875819315\n",
      "0.38206381826185015\n",
      "0.3768129040436998\n",
      "0.3716675211089441\n",
      "0.3666207309622178\n",
      "0.3616661521093227\n",
      "0.35679793076312016\n",
      "0.3520107102118215\n",
      "0.3472995992787776\n",
      "0.342660140573553\n",
      "0.33808827922528983\n",
      "0.33358033265464726\n",
      "0.32913296176963197\n",
      "0.3247431438105614\n",
      "0.3204081469405413\n",
      "0.31612550658498084\n",
      "0.3118930034635018\n",
      "0.30770864322354885\n",
      "0.30357063756986147\n",
      "0.2994773867812932\n",
      "0.29542746351118576\n",
      "0.2914195977759251\n",
      "0.28745266304595046\n",
      "0.28352566336290524\n",
      "0.2796377214149458\n",
      "0.27578806750925006\n",
      "0.27197602938639975\n",
      "0.26820102282571956\n",
      "0.26446254299408645\n",
      "0.26076015649335393\n",
      "0.2570934940636421\n",
      "0.2534622439014442\n",
      "0.24986614555300252\n",
      "0.24630498434475714\n",
      "0.24277858631398389\n",
      "0.23928681360403006\n",
      "0.2358295602898996\n",
      "0.23240674860126143\n",
      "0.22901832551137422\n",
      "0.2256642596618061\n",
      "0.2223445385942724\n",
      "0.21905916626235106\n",
      "0.2158081607972619\n",
      "0.21259155250332856\n",
      "0.2094093820601196\n",
      "0.20626169890966006\n",
      "0.20314855980841195\n",
      "0.20007002752503233\n",
      "0.197026169666171\n",
      "0.19401705761377563\n",
      "0.19104276555855138\n",
      "0.1881033696153549\n",
      "0.18519894700739312\n",
      "0.18232957530716365\n",
      "0.17949533172310395\n",
      "0.1766962924219152\n",
      "0.173932531877503\n",
      "0.17120412223843867\n",
      "0.16851113270677665\n",
      "0.16585362892197827\n",
      "0.16323167234461727\n",
      "0.16064531963541134\n",
      "0.15809462202602537\n",
      "0.15557962467895534\n",
      "0.1531003660346587\n",
      "0.15065687714494375\n",
      "0.14824918099245615\n",
      "0.14587729179691533\n",
      "0.14354121430953\n",
      "0.14124094309778182\n",
      "0.13897646182349294\n",
      "0.13674774251776414\n",
      "0.13455474485701427\n",
      "0.13239741544491906\n",
      "0.13027568710557652\n",
      "0.12818947819365517\n",
      "0.1261386919276657\n",
      "0.12412321575276894\n",
      "0.12214292073972477\n",
      "0.12019766102669309\n",
      "0.11828727331057864\n",
      "0.11641157639450991\n",
      "0.11457037079782428\n",
      "0.11276343843461202\n",
      "0.11099054236644315\n",
      "0.10925142663438143\n",
      "0.10754581617476608\n",
      "0.1058734168225346\n",
      "0.10423391540508994\n",
      "0.10262697992884423\n",
      "0.10105225985969318\n",
      "0.09950938649771775\n",
      "0.09799797344545333\n",
      "0.09651761716809067\n",
      "0.09506789764300773\n",
      "0.09364837909509188\n",
      "0.0922586108134063\n",
      "0.09089812804391215\n",
      "0.08956645295217297\n",
      "0.08826309564927191\n",
      "0.08698755527356344\n",
      "0.08573932112037451\n",
      "0.0845178738113641\n",
      "0.08332268649496533\n",
      "0.08215322606915182\n",
      "0.08100895441770706\n",
      "0.07988932965122625\n",
      "0.07879380734422498\n",
      "0.07772184175999401\n",
      "0.07667288705517825\n",
      "0.07564639845649522\n",
      "0.07464183340251469\n",
      "0.07365865264398622\n",
      "0.07269632129682646\n",
      "0.07175430984253583\n",
      "0.07083209507150838\n",
      "0.06992916096539373\n",
      "0.06904499951539507\n",
      "0.0681791114740796\n",
      "0.06733100703897987\n",
      "0.06650020646692983\n",
      "0.06568624061871352\n",
      "0.06488865143421621\n",
      "0.06410699233881638\n",
      "0.06334082858227559\n",
      "0.06258973751184832\n",
      "0.06185330878173984\n",
      "0.061131144501407046\n",
      "0.060422859325498436\n",
      "0.05972808048848785\n",
      "0.05904644778726262\n",
      "0.05837761351507823\n",
      "0.05772124235040891\n",
      "0.05707701120428554\n",
      "0.056444609029745914\n",
      "0.0558237365970096\n",
      "0.05521410623795339\n",
      "0.05461544156339149\n",
      "0.05402747715657368\n",
      "0.05344995824619428\n",
      "0.05288264036207788\n",
      "0.05232528897655137\n",
      "0.05177767913435952\n",
      "0.05123959507380869\n",
      "0.05071082984165001\n",
      "0.05019118490403372\n",
      "0.04968046975569391\n",
      "0.04917850152933386\n",
      "0.04868510460701656\n",
      "0.04820011023518675\n",
      "0.04772335614478715\n",
      "0.047254686177768526\n",
      "0.04679394992114259\n",
      "0.04634100234957818\n",
      "0.04589570347740625\n",
      "0.04545791802076818\n",
      "0.045027515070523313\n",
      "0.04460436777641919\n",
      "0.04418835304292676\n",
      "0.043779351237049194\n",
      "0.04337724590832636\n",
      "0.04298192352118121\n",
      "0.04259327319968816\n",
      "0.04221118648477663\n",
      "0.04183555710383593\n",
      "0.04146628075263335\n",
      "0.04110325488942583\n",
      "0.04074637854109888\n",
      "0.040395552121150244\n",
      "0.04005067725929956\n",
      "0.03971165664249621\n",
      "0.03937839386707029\n",
      "0.03905079330177156\n",
      "0.0387287599614244\n",
      "0.038412199390919326\n",
      "0.03810101755926823\n",
      "0.0377951207634352\n",
      "0.03749441554166569\n",
      "0.037198808596030065\n",
      "0.0369082067239064\n",
      "0.036622516758125626\n",
      "0.03634164551550479\n",
      "0.036065499753504524\n",
      "0.035793986134742474\n",
      "0.03552701119910101\n",
      "0.03526448134317372\n",
      "0.03500630280679183\n",
      "0.03475238166638023\n",
      "0.03450262383488807\n",
      "0.034256935068045676\n",
      "0.03401522097669571\n",
      "0.033777387044947316\n",
      "0.033543338653900766\n",
      "0.03331298111068832\n",
      "0.03308621968257404\n",
      "0.03286295963585334\n",
      "0.03264310627928666\n",
      "0.03242656501180242\n",
      "0.03221324137419425\n",
      "0.0320030411045388\n",
      "0.03179587019705088\n",
      "0.031591634964091925\n",
      "0.03139024210104052\n",
      "0.03119159875373166\n",
      "0.030995612588165765\n",
      "0.030802191862187633\n",
      "0.030611245498828925\n",
      "0.030422683161010912\n",
      "0.03023641532729593\n",
      "0.030052353368385683\n",
      "0.029870409624052324\n",
      "0.02969049748020251\n",
      "0.02951253144576659\n",
      "0.029336427229116606\n",
      "0.02916210181371754\n",
      "0.028989473532721133\n",
      "0.028818462142222844\n",
      "0.028648988892908015\n",
      "0.028480976599820563\n",
      "0.028314349710002625\n",
      "0.02814903436776146\n",
      "0.027984958477331352\n",
      "0.02782205176271573\n",
      "0.027660245824503243\n",
      "0.027499474193471263\n",
      "0.02733967238080083\n",
      "0.027180777924746876\n",
      "0.027022730433621432\n",
      "0.026865471624965605\n",
      "0.026708945360801886\n",
      "0.02655309767887617\n",
      "0.026397876819816302\n",
      "0.02624323325015003\n",
      "0.026089119681143393\n",
      "0.025935491083436693\n",
      "0.025782304697471524\n",
      "0.025629520039719488\n",
      "0.025477098904737833\n",
      "0.02532500536309136\n",
      "0.02517320575519805\n",
      "0.025021668681165322\n",
      "0.024870364986700103\n",
      "0.024719267745186152\n",
      "0.024568352236035634\n",
      "0.02441759591943054\n",
      "0.02426697840758123\n",
      "0.024116481432638233\n",
      "0.02396608881139921\n",
      "0.02381578640696326\n",
      "0.023665562087491057\n",
      "0.023515405682230747\n",
      "0.023365308934979616\n",
      "0.02321526545515179\n",
      "0.02306527066662591\n",
      "0.02291532175454907\n",
      "0.022765417610275764\n",
      "0.022615558774617864\n",
      "0.02246574737958675\n",
      "0.02231598708880112\n",
      "0.02216628303673996\n",
      "0.022016641767011843\n",
      "0.02186707116981346\n",
      "0.021717580418744224\n",
      "0.021568179907140494\n",
      "0.021418881184092874\n",
      "0.021269696890298267\n",
      "0.02112064069389972\n",
      "0.02097172722646072\n",
      "0.020822972019210455\n",
      "0.02067439143969799\n",
      "0.020526002628982055\n",
      "0.020377823439478977\n",
      "0.020229872373584694\n",
      "0.020082168523180084\n",
      "0.019934731510123847\n",
      "0.019787581427827283\n",
      "0.019640738784003308\n",
      "0.01949422444467143\n",
      "0.019348059579495626\n",
      "0.019202265608526246\n",
      "0.019056864150408852\n",
      "0.018911876972118186\n",
      "0.01876732594026765\n",
      "0.018623232974040636\n",
      "0.018479619999781337\n",
      "0.01833650890728013\n",
      "0.018193921507779334\n",
      "0.01805187949372249\n",
      "0.017910404400264528\n",
      "0.017769517568552582\n",
      "0.017629240110787182\n",
      "0.017489592877061887\n",
      "0.01735059642398392\n",
      "0.01721227098506424\n",
      "0.01707463644286966\n",
      "0.016937712302920828\n",
      "0.016801517669318554\n",
      "0.016666071222075377\n",
      "0.016531391196130195\n",
      "0.016397495362015433\n",
      "0.0162644010081475\n",
      "0.016132124924707753\n",
      "0.016000683389077442\n",
      "0.015870092152790477\n",
      "0.015740366429963258\n",
      "0.015611520887161657\n",
      "0.015483569634661127\n",
      "0.015356526219058097\n",
      "0.015230403617184706\n",
      "0.01510521423128392\n",
      "0.014980969885395802\n",
      "0.014857681822908041\n",
      "0.014735360705224576\n",
      "0.014614016611501218\n",
      "0.014493659039402404\n",
      "0.014374296906828885\n",
      "0.014255938554569525\n",
      "0.014138591749827975\n",
      "0.01402226369057731\n",
      "0.013906961010694694\n",
      "0.013792689785830151\n",
      "0.01367945553996272\n",
      "0.013567263252598764\n",
      "0.013456117366568598\n",
      "0.013346021796377073\n",
      "0.013236979937065735\n",
      "0.013128994673545204\n",
      "0.013022068390356589\n",
      "0.012916202981824127\n",
      "0.012811399862558168\n",
      "0.0127076599782741\n",
      "0.012604983816890025\n",
      "0.012503371419868974\n",
      "0.012402822393771908\n",
      "0.01230333592199116\n",
      "0.012204910776631151\n",
      "0.012107545330509466\n",
      "0.012011237569248823\n",
      "0.011915985103433217\n",
      "0.011821785180804411\n",
      "0.011728634698471884\n",
      "0.011636530215116087\n",
      "0.011545467963161724\n",
      "0.011455443860902066\n",
      "0.01136645352455316\n",
      "0.011278492280222466\n",
      "0.01119155517577384\n",
      "0.011105636992573014\n",
      "0.011020732257099734\n",
      "0.010936835252413627\n",
      "0.010853940029460177\n",
      "0.010772040418207061\n",
      "0.010691130038598413\n",
      "0.010611202311321156\n",
      "0.01053225046837068\n",
      "0.010454267563411961\n",
      "0.01037724648192753\n",
      "0.010301179951147087\n",
      "0.010226060549753202\n",
      "0.010151880717359556\n",
      "0.010078632763757431\n",
      "0.01000630887792739\n",
      "0.009934901136814719\n",
      "0.009864401513866016\n",
      "0.009794801887325986\n",
      "0.009726094048294458\n",
      "0.009658269708542052\n",
      "0.009591320508087267\n",
      "0.009525238022533217\n",
      "0.00946001377016683\n",
      "0.00939563921882168\n",
      "0.009332105792506552\n",
      "0.00926940487780185\n",
      "0.009207527830026638\n",
      "0.009146465979178633\n",
      "0.009086210635651444\n",
      "0.009026753095731097\n",
      "0.00896808464687658\n",
      "0.00891019657278672\n",
      "0.008853080158259231\n",
      "0.008796726693844459\n",
      "0.00874112748029792\n",
      "0.008686273832837536\n",
      "0.008632157085208854\n",
      "0.008578768593561345\n",
      "0.008526099740143971\n",
      "0.008474141936820626\n",
      "0.008422886628412828\n",
      "0.00837232529587293\n",
      "0.00832244945929301\n",
      "0.008273250680754119\n",
      "0.008224720567020255\n",
      "0.008176850772082024\n",
      "0.008129632999554031\n",
      "0.008083059004931772\n",
      "0.008037120597710834\n",
      "0.007991809643374771\n",
      "0.00794711806525406\n",
      "0.007903037846263186\n",
      "0.007859561030516611\n",
      "0.007816679724831528\n",
      "0.00777438610011935\n",
      "0.007732672392669894\n",
      "0.007691530905334304\n",
      "0.00765095400860815\n",
      "0.007610934141620283\n",
      "0.007571463813030931\n",
      "0.007532535601841529\n",
      "0.007494142158121934\n",
      "0.0074562762036564445\n",
      "0.007418930532513904\n",
      "0.007382098011543476\n",
      "0.007345771580800702\n",
      "0.007309944253906367\n",
      "0.007274609118341294\n",
      "0.007239759335680059\n",
      "0.007205388141766297\n",
      "0.007171488846833324\n",
      "0.007138054835571009\n",
      "0.007105079567143682\n",
      "0.007072556575159322\n",
      "0.007040479467594892\n",
      "0.00700884192667755\n",
      "0.006977637708727037\n",
      "0.006946860643957969\n",
      "0.006916504636247619\n",
      "0.006886563662868451\n",
      "0.006857031774189124\n",
      "0.006827903093345348\n",
      "0.006799171815881661\n",
      "0.006770832209367068\n",
      "0.006742878612985898\n",
      "0.006715305437104796\n",
      "0.006688107162817943\n",
      "0.0066612783414723865\n",
      "0.006634813594174242\n",
      "0.006608707611277088\n",
      "0.006582955151854321\n",
      "0.006557551043157254\n",
      "0.006532490180057686\n",
      "0.006507767524480019\n",
      "0.006483378104819845\n",
      "0.006459317015353517\n",
      "0.006435579415637195\n",
      "0.0064121605298979945\n",
      "0.006389055646417137\n",
      "0.006366260116906288\n",
      "0.006343769355877961\n",
      "0.0063215788400110265\n",
      "0.006299684107511064\n",
      "0.006278080757467521\n",
      "0.006256764449207728\n",
      "0.0062357309016478875\n",
      "0.006214975892642963\n",
      "0.006194495258334517\n",
      "0.006174284892498314\n",
      "0.006154340745891527\n",
      "0.0061346588255995075\n",
      "0.006115235194384231\n",
      "0.006096065970033196\n",
      "0.006077147324709524\n",
      "0.006058475484304755\n",
      "0.0060400467277928254\n",
      "0.006021857386587394\n",
      "0.006003903843901263\n",
      "0.005986182534109779\n",
      "0.005968689942116321\n",
      "0.005951422602723013\n",
      "0.005934377100003814\n",
      "0.00591755006668246\n",
      "0.005900938183515008\n",
      "0.005884538178675417\n",
      "0.0058683468271474954\n",
      "0.0058523609501204684\n",
      "0.005836577414389813\n",
      "0.005820993131763111\n",
      "0.005805605058471361\n",
      "0.005790410194584961\n",
      "0.0057754055834355646\n",
      "0.005760588311043294\n",
      "0.005745955505549557\n",
      "0.005731504336654982\n",
      "0.00571723201506391\n",
      "0.005703135791933678\n",
      "0.00568921295833049\n",
      "0.005675460844690354\n",
      "0.005661876820286417\n",
      "0.005648458292701854\n",
      "0.0056352027073088415\n",
      "0.005622107546753005\n",
      "0.0056091703304444376\n",
      "0.00559638861405398\n",
      "0.00558375998901562\n",
      "0.005571282082034973\n",
      "0.005558952554603361\n",
      "0.005546769102517558\n",
      "0.005534729455406358\n",
      "0.005522831376261429\n",
      "0.005511072660975478\n",
      "0.005499451137885013\n",
      "0.005487964667319552\n",
      "0.005476611141156068\n",
      "0.005465388482379507\n",
      "0.005454294644648529\n",
      "0.00544332761186727\n",
      "0.0054324853977619405\n",
      "0.005421766045463801\n",
      "0.005411167627096879\n",
      "0.005400688243371464\n",
      "0.005390326023182693\n",
      "0.005380079123214681\n",
      "0.005369945727549676\n",
      "0.005359924047282734\n",
      "0.0053500123201411405\n",
      "0.005340208810109049\n",
      "0.005330511807057589\n",
      "0.005320919626379208\n",
      "0.005311430608627497\n",
      "0.005302043119161767\n",
      "0.005292755547796318\n",
      "0.005283566308454661\n",
      "0.005274473838828149\n",
      "0.005265476600039868\n",
      "0.005256573076312252\n",
      "0.0052477617746399625\n",
      "0.005239041224467103\n",
      "0.005230409977368522\n",
      "0.005221866606735884\n",
      "0.005213409707468004\n",
      "0.005205037895665104\n",
      "0.005196749808327697\n",
      "0.0051885441030596795\n",
      "0.00518041945777486\n",
      "0.005172374570408319\n",
      "0.005164408158631565\n",
      "0.005156518959571011\n",
      "0.005148705729531382\n",
      "0.0051409672437221585\n",
      "0.005133302295988005\n",
      "0.005125709698543348\n",
      "0.005118188281709806\n",
      "0.005110736893658194\n",
      "0.005103354400153392\n",
      "0.005096039684303099\n",
      "0.005088791646310082\n",
      "0.005081609203227691\n",
      "0.005074491288718908\n",
      "0.005067436852818647\n",
      "0.005060444861699353\n",
      "0.005053514297440166\n",
      "0.005046644157798726\n",
      "0.0050398334559865355\n",
      "0.005033081220447443\n",
      "0.00502638649463916\n",
      "0.005019748336817484\n",
      "0.005013165819824085\n",
      "0.005006638030876764\n",
      "0.005000164071362962\n",
      "0.00499374305663577\n",
      "0.004987374115813251\n",
      "0.00498105639158005\n",
      "0.004974789039992017\n",
      "0.004968571230283466\n",
      "0.004962402144677367\n",
      "0.004956280978197625\n",
      "0.004950206938484421\n",
      "0.004944179245611904\n",
      "0.004938197131908718\n",
      "0.004932259841780341\n",
      "0.004926366631534581\n",
      "0.004920516769209219\n",
      "0.004914709534401896\n",
      "0.004908944218102512\n",
      "0.004903220122528234\n",
      "0.004897536560959974\n",
      "0.004891892857581876\n",
      "0.004886288347322987\n",
      "0.0048807223757005\n",
      "0.004875194298665917\n",
      "0.004869703482452721\n",
      "0.0048642493034267665\n",
      "0.004858831147938123\n",
      "0.004853448412175271\n",
      "0.004848100502021483\n",
      "0.0048427868329128\n",
      "0.004837506829698293\n",
      "0.004832259926502132\n",
      "0.004827045566587393\n",
      "0.004821863202222318\n",
      "0.0048167122945474315\n",
      "0.004811592313445733\n",
      "0.004806502737413558\n",
      "0.004801443053434108\n",
      "0.0047964127568518545\n",
      "0.004791411351249593\n",
      "0.004786438348326568\n",
      "0.004781493267778625\n",
      "0.004776575637179659\n",
      "0.004771684991865063\n",
      "0.0047668208748166754\n",
      "0.00476198283654925\n",
      "0.004757170434998541\n",
      "0.004752383235411144\n",
      "0.004747620810235242\n",
      "0.004742882739013708\n",
      "0.004738168608278171\n",
      "0.004733478011444416\n",
      "0.004728810548709944\n",
      "0.00472416582695207\n",
      "0.00471954345962799\n",
      "0.004714943066676209\n",
      "0.004710364274419122\n",
      "0.00470580671546709\n",
      "0.0047012700286239335\n",
      "0.004696753858793402\n",
      "0.004692257856887431\n",
      "0.00468778167973526\n",
      "0.004683324989994054\n",
      "0.0046788874560607895\n",
      "0.004674468751985034\n",
      "0.0046700685573834045\n",
      "0.004665686557355071\n",
      "0.004661322442398216\n",
      "0.004656975908327822\n",
      "0.004652646656194822\n",
      "0.004648334392205943\n",
      "0.0046440388276451534\n",
      "0.004639759678795732\n",
      "0.004635496666863947\n",
      "0.004631249517903158\n",
      "0.004627017962739742\n",
      "0.004622801736899451\n",
      "0.004618600580535258\n",
      "0.0046144142383555626\n",
      "0.004610242459554298\n",
      "0.004606084997741491\n",
      "0.004601941610874621\n",
      "0.004597812061191588\n",
      "0.004593696115143716\n",
      "0.0045895935433310575\n",
      "0.004585504120436968\n",
      "0.004581427625164967\n",
      "0.004577363840175756\n",
      "0.004573312552025352\n",
      "0.004569273551104323\n",
      "0.0045652466315773\n",
      "0.004561231591323975\n",
      "0.004557228231880606\n",
      "0.004553236358382418\n",
      "0.004549255779506794\n",
      "0.004545286307417401\n",
      "0.004541327757709023\n",
      "0.0045373799493532\n",
      "0.004533442704644544\n",
      "0.004529515849148147\n",
      "0.004525599211647257\n",
      "0.004521692624092176\n",
      "0.004517795921549725\n",
      "0.004513908942153087\n",
      "0.0045100315270530465\n",
      "0.004506163520369528\n",
      "0.004502304769143651\n",
      "0.0044984551232909115\n",
      "0.004494614435554886\n",
      "0.004490782561461428\n",
      "0.004486959359273611\n",
      "0.004483144689947769\n",
      "0.004479338417089233\n",
      "0.0044755404069097135\n",
      "0.004471750528184693\n",
      "0.004467968652211487\n",
      "0.004464194652768392\n",
      "0.004460428406073668\n",
      "0.004456669790745819\n",
      "0.00445291868776404\n",
      "0.004449174980429395\n",
      "0.004445438554326555\n",
      "0.004441709297286174\n",
      "0.004437987099347459\n",
      "0.00443427185272202\n",
      "0.004430563451757298\n",
      "0.004426861792901449\n",
      "0.00442316677466801\n",
      "0.004419478297601729\n",
      "0.0044157962642442415\n",
      "0.004412120579100993\n",
      "0.004408451148607933\n",
      "0.004404787881099059\n",
      "0.004401130686774767\n",
      "0.004397479477669799\n",
      "0.004393834167622764\n",
      "0.0043901946722448544\n",
      "0.0043865609088905915\n",
      "0.004382932796627266\n",
      "0.004379310256206367\n",
      "0.004375693210034546\n",
      "0.0043720815821450525\n",
      "0.004368475298170217\n",
      "0.004364874285313572\n",
      "0.0043612784723231586\n",
      "0.004357687789464255\n",
      "0.0043541021684937965\n",
      "0.004350521542633908\n",
      "0.004346945846546722\n",
      "0.004343375016309195\n",
      "0.004339808989388495\n",
      "0.004336247704617428\n",
      "0.0043326911021706925\n",
      "0.00432913912354115\n",
      "0.004325591711516577\n",
      "0.004322048810156981\n",
      "0.004318510364771723\n",
      "0.004314976321897591\n",
      "0.004311446629276953\n",
      "0.004307921235836063\n",
      "0.00430440009166398\n",
      "0.004300883147991724\n",
      "0.004297370357171849\n",
      "0.00429386167265804\n",
      "0.004290357048985195\n",
      "0.0042868564417501516\n",
      "0.004283359807592093\n",
      "0.004279867104173631\n",
      "0.0042763782901621234\n",
      "0.004272893325211373\n",
      "0.0042694121699435325\n",
      "0.00426593478593095\n",
      "0.004262461135679045\n",
      "0.004258991182608833\n",
      "0.004255524891039962\n",
      "0.004252062226173901\n",
      "0.004248603154077659\n",
      "0.004245147641667275\n",
      "0.004241695656692329\n",
      "0.00423824716771975\n",
      "0.00423480214411873\n",
      "0.004231360556045276\n",
      "0.0042279223744273444\n",
      "0.00422448757095023\n",
      "0.00422105611804201\n",
      "0.004217627988858998\n",
      "0.0042142031572721984\n",
      "0.004210781597853234\n",
      "0.004207363285860789\n",
      "0.004203948197227222\n",
      "0.004200536308545471\n",
      "0.004197127597056189\n",
      "0.00419372204063484\n",
      "0.004190319617779285\n",
      "0.0041869203075973575\n",
      "0.004183524089795107\n",
      "0.004180130944664376\n",
      "0.004176740853071339\n",
      "0.004173353796444984\n",
      "0.004169969756765712\n",
      "0.004166588716554169\n",
      "0.004163210658860172\n",
      "0.004159835567252272\n",
      "0.004156463425806652\n",
      "0.004153094219096983\n",
      "0.004149727932184059\n",
      "0.004146364550605713\n",
      "0.004143004060366985\n",
      "0.004139646447930134\n",
      "0.00413629170020504\n",
      "0.004132939804540275\n",
      "0.004129590748712806\n",
      "0.004126244520919989\n",
      "0.004122901109769663\n",
      "0.004119560504271913\n",
      "0.004116222693830271\n",
      "0.004112887668232679\n",
      "0.004109555417643931\n",
      "0.004106225932596851\n",
      "0.004102899203984241\n",
      "0.004099575223051109\n",
      "0.004096253981386615\n",
      "0.0040929354709165625\n",
      "0.004089619683895681\n",
      "0.004086306612900204\n",
      "0.004082996250820641\n",
      "0.004079688590854516\n",
      "0.00407638362649915\n",
      "0.004073081351545223\n",
      "0.00406978176006927\n",
      "0.00406648484642744\n",
      "0.00406319060524884\n",
      "0.004059899031428903\n",
      "0.004056610120123169\n",
      "0.0040533238667408746\n",
      "0.004050040266939073\n",
      "0.004046759316616331\n",
      "0.004043481011907008\n",
      "0.0040402053491751955\n",
      "0.0040369323250093354\n",
      "0.0040336619362161905\n",
      "0.0040303941798157435\n",
      "0.004027129053035465\n",
      "0.004023866553305007\n",
      "0.0040206066782512775\n",
      "0.0040173494256929496\n",
      "0.004014094793635601\n",
      "0.004010842780266671\n",
      "0.004007593383950598\n",
      "0.0040043466032242975\n",
      "0.004001102436792058\n",
      "0.003997860883521256\n",
      "0.003994621942437636\n",
      "0.0039913856127211525\n",
      "0.003988151893701183\n",
      "0.0039849207848526415\n",
      "0.00398169228579162\n",
      "0.003978466396271301\n",
      "0.003975243116177872\n",
      "0.003972022445526581\n",
      "0.003968804384457875\n",
      "0.0039655889332336006\n",
      "0.003962376092233143\n",
      "0.003959165861949865\n",
      "0.003955958242987438\n",
      "0.003952753236056263\n",
      "0.003949550841970194\n",
      "0.0039463510616428304\n",
      "0.00394315389608444\n",
      "0.003939959346398536\n",
      "0.003936767413778764\n",
      "0.003933578099505567\n",
      "0.0039303914049433585\n",
      "0.003927207331537277\n",
      "0.003924025880810306\n",
      "0.003920847054360349\n",
      "0.00391767085385737\n",
      "0.003914497281040553\n",
      "0.0039113263377157466\n",
      "0.003908158025752451\n",
      "0.003904992347081452\n",
      "0.0039018293036921117\n",
      "0.003898668897629873\n",
      "0.0038955111309939288\n",
      "0.003892356005934345\n",
      "0.003889203524650219\n",
      "0.0038860536893870214\n",
      "0.0038829065024344493\n",
      "0.0038797619661240118\n",
      "0.0038766200828271166\n",
      "0.003873480854952754\n",
      "0.0038703442849452913\n",
      "0.0038672103752826464\n",
      "0.003864079128474312\n",
      "0.0038609505470589625\n",
      "0.0038578246336030005\n",
      "0.0038547013906984223\n",
      "0.003851580820961095\n",
      "0.0038484629270287363\n",
      "0.0038453477115595264\n",
      "0.0038422351772300152\n",
      "0.003839125326733565\n",
      "0.003836018162778736\n",
      "0.0038329136880876054\n",
      "0.003829811905394254\n",
      "0.0038267128174432428\n",
      "0.0038236164269879525\n",
      "0.0038205227367891963\n",
      "0.0038174317496138263\n",
      "0.00381434346823333\n",
      "0.0038112578954222825\n",
      "0.003808175033957288\n",
      "0.0038050948866153716\n",
      "0.003802017456172859\n",
      "0.0037989427454042076\n",
      "0.0037958707570805005\n",
      "0.003792801493968582\n",
      "0.00378973495882964\n",
      "0.003786671154418216\n",
      "0.0037836100834811237\n",
      "0.0037805517487561513\n",
      "0.0037774961529712815\n",
      "0.0037744432988435013\n",
      "0.0037713931890778637\n",
      "0.0037683458263664646\n",
      "0.003765301213387481\n",
      "0.0037622593528044463\n",
      "0.0037592202472650875\n",
      "0.003756183899400488\n",
      "0.003753150311824388\n",
      "0.0037501194871322604\n",
      "0.0037470914279005394\n",
      "0.003744066136685711\n",
      "0.0037410436160236985\n",
      "0.0037380238684290505\n",
      "0.003735006896394298\n",
      "0.0037319927023890705\n",
      "0.003728981288859655\n",
      "0.003725972658228118\n",
      "0.0037229668128918782\n",
      "0.0037199637552227743\n",
      "0.0037169634875669232\n",
      "0.0037139660122436084\n",
      "0.0037109713315451502\n",
      "0.0037079794477361356\n",
      "0.003704990363052943\n",
      "0.003702004079703148\n",
      "0.003699020599865234\n",
      "0.0036960399256878605\n",
      "0.003693062059289588\n",
      "0.003690087002758281\n",
      "0.00368711475815084\n",
      "0.0036841453274926712\n",
      "0.0036811787127773725\n",
      "0.003678214915966085\n",
      "0.003675253938987587\n",
      "0.0036722957837374962\n",
      "0.003669340452078139\n",
      "0.0036663879458382325\n",
      "0.0036634382668124803\n",
      "0.0036604914167613174\n",
      "0.003657547397410571\n",
      "0.0036546062104513286\n",
      "0.0036516678575394266\n",
      "0.0036487323402954247\n",
      "0.0036457996603042908\n",
      "0.0036428698191150965\n",
      "0.00363994281824086\n",
      "0.0036370186591584094\n",
      "0.0036340973433080454\n",
      "0.0036311788720934436\n",
      "0.0036282632468814975\n",
      "0.0036253504690021174\n",
      "0.0036224405397480455\n",
      "0.0036195334603748267\n",
      "0.0036166292321004176\n",
      "0.003613727856105481\n",
      "0.0036108293335328284\n",
      "0.003607933665487689\n",
      "0.0036050408530373204\n",
      "0.0036021508972110794\n",
      "0.0035992637990003465\n",
      "0.003596379559358356\n",
      "0.00359349817920026\n",
      "0.0035906196594029036\n",
      "0.0035877440008050127\n",
      "0.0035848712042069577\n",
      "0.0035820012703708043\n",
      "0.003579134200020224\n",
      "0.00357626999384074\n",
      "0.0035734086524791646\n",
      "0.0035705501765443152\n",
      "0.0035676945666063566\n",
      "0.0035648418231973627\n",
      "0.003561991946810801\n",
      "0.0035591449379020284\n",
      "0.0035563007968880916\n",
      "0.0035534595241477622\n",
      "0.0035506211200216674\n"
     ]
    }
   ],
   "source": [
    "# Implement a Simple RNN with Backpropagation Through Time (BPTT)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\n",
    "expected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n",
    "\n",
    "def grad(y_pred, y):\n",
    "    return y_pred - y\n",
    "\n",
    "def mse(y_pred, y):\n",
    "    return np.mean(np.pow(y_pred - y, 2))\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the RNN with random weights and zero biases.\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN for a given sequence of inputs.\n",
    "        \"\"\"\n",
    "        timestamps, input_size = x.shape\n",
    "        cache = {\n",
    "            \"hidden_states\": [],\n",
    "            \"y_pred\": []\n",
    "        }\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for timestamp in range(timestamps):\n",
    "            h = np.tanh(self.W_xh @ x[timestamp].reshape(-1, 1) + self.W_hh @ h + self.b_h)\n",
    "            y = self.W_hy @ h + self.b_y\n",
    "            cache[\"hidden_states\"].append(h.copy())\n",
    "            cache[\"y_pred\"].append(y.copy())\n",
    "        self.cache = cache.copy()\n",
    "        return cache\n",
    "\n",
    "\n",
    "    def backward(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Backpropagation through time to adjust weights based on error gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize empty fields\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "        h_ts = self.cache[\"hidden_states\"]\n",
    "        y_ts = self.cache[\"y_pred\"]\n",
    "        timestamps, input_size = x.shape\n",
    "        loss = 0.0\n",
    "        full_loss = []\n",
    "        for t in reversed(range(timestamps)):\n",
    "            dL_dy = grad(y_ts[t], y[t].reshape(-1, 1))\n",
    "            loss += mse(y_ts[t], y[t].reshape(-1, 1))\n",
    "            dW_hy += dL_dy @ h_ts[t].T\n",
    "            db_y += dL_dy\n",
    "            dL_dh = self.W_hy.T @ dL_dy + dh_next\n",
    "\n",
    "            dtanh = (1 - self.cache[\"hidden_states\"][t] ** 2) * dL_dh\n",
    "\n",
    "            dW_hh += dtanh @ h_ts[t - 1].T\n",
    "            dW_xh += dtanh @ x[t].reshape(-1, 1)\n",
    "            db_h += dtanh\n",
    "            dh_next = self.W_hh.T @ dtanh\n",
    "        full_loss.append(loss / timestamps)\n",
    "        print(loss / timestamps)\n",
    "        \n",
    "        self.W_xh -= learning_rate * dW_xh\n",
    "        self.W_hh -= learning_rate * dW_hh\n",
    "        self.W_hy -= learning_rate * dW_hy\n",
    "        self.b_h -= learning_rate * db_h\n",
    "        self.b_y -= learning_rate * db_y\n",
    "\n",
    "rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n",
    "for _ in range(1000):\n",
    "    output = rnn.forward(input_sequence)\n",
    "    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n",
    "    # print(output[\"y_pred\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Calculator\n",
    "\n",
    "position = 2, d_model = 8\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pos_encoding(position: int, d_model: int):\n",
    "    # Your code here\n",
    "    \n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    pos_encoding = np.float16(pos_encoding)\n",
    "\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103., 109.,  46.,  99.],\n",
       "       [103., 109.,  46.,  99.],\n",
       "       [103., 109.,  46.,  99.],\n",
       "       [103., 109.,  46.,  99.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement Multi-Head Attention\n",
    "import numpy as np\n",
    "\n",
    "m, n = 4, 4 \n",
    "n_heads = 2 \n",
    "np.random.seed(42) \n",
    "X = np.arange(m*n).reshape(m,n)\n",
    "X = np.random.permutation(X.flatten()).reshape(m, n)\n",
    "W_q = np.random.randint(0,4,size=(n,n))\n",
    "W_k = np.random.randint(0,5,size=(n,n))\n",
    "W_v = np.random.randint(0,6,size=(n,n))\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v) \n",
    "\n",
    "def softmax(X):\n",
    "    x_max = np.max(X, axis=-1, keepdims=True)  # для стабильности\n",
    "    return np.exp(X - x_max) / np.sum(np.exp(X - x_max), axis=-1, keepdims=True)\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    Q = X @ W_q\n",
    "    K = X @ W_k\n",
    "    V = X @ W_v\n",
    "    return Q, K, V\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q.shape = (seq_len, d_model)\n",
    "    K.shape = (seq_len, d_model)\n",
    "    V.shape = (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    d_model = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_model)\n",
    "    soft_scores = softmax(scores)\n",
    "    scores = np.matmul(soft_scores, V)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def multi_head_attention(Q, K, V, n_heads):\n",
    "    seq_len, d_model = Q.shape\n",
    "    assert d_model % n_heads == 0\n",
    "    d_k = d_model // n_heads\n",
    "    \"\"\"\n",
    "    Каждая голова должна работать со всеми токенами, но с меньшей размерностью признаков\n",
    "    Q = np.array([\n",
    "        [1, 2, 3, 4],\n",
    "        [5, 6, 7, 8],\n",
    "        [9, 10, 11, 12]\n",
    "    ])  # Q.shape = (3, 4)\n",
    "\n",
    "    n_heads = 2\n",
    "    d_k = Q.shape[1] // n_heads\n",
    "\n",
    "    Q_h = Q.reshape(3, 2, 2).transpose(1, 0, 2)\n",
    "    print(Q_h.shape)\n",
    "\n",
    "    Q_h:\n",
    "    [\n",
    "        [[ 1, 2], [5, 6], [9, 10]],  # Первая голова\n",
    "        [[ 3, 4], [7, 8], [11, 12]]  # Вторая голова\n",
    "    ]\n",
    "    \"\"\"\n",
    "    Q_h = Q.reshape(seq_len, n_heads, -1).transpose(1, 0, 2)\n",
    "    K_h = K.reshape(seq_len, n_heads, -1).transpose(1, 0, 2)\n",
    "    V_h = V.reshape(seq_len, n_heads, -1).transpose(1, 0, 2)\n",
    "    full_score = np.array([self_attention(q, k, v) for q, k, v in zip(Q_h, K_h, V_h)])\n",
    "    # Чтобы сконкатить обратно, нужно сделать тот же порядок действий\n",
    "    attn = full_score.transpose(1, 0, 2).reshape(seq_len, d_k * n_heads)\n",
    "    \n",
    "    return attn\n",
    "\n",
    "multi_head_attention(Q, K, V, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[547. 490. 399. 495. 485. 439. 645. 393.]\n",
      " [547. 490. 399. 495. 485. 439. 645. 393.]\n",
      " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
      " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
      " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
      " [471. 472. 429. 538. 377. 450. 531. 362.]]\n"
     ]
    }
   ],
   "source": [
    "# Implement Masked Self-Attention\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) \n",
    "X = np.arange(48).reshape(6,8)\n",
    "X = np.random.permutation(X.flatten()).reshape(6, 8) \n",
    "mask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\n",
    "W_q = np.random.randint(0,4,size=(8,8))\n",
    "W_k = np.random.randint(0,5,size=(8,8))\n",
    "W_v = np.random.randint(0,6,size=(8,8)) \n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v) \n",
    "\n",
    "def softmax(X):\n",
    "    x_max = np.max(X, axis=-1, keepdims=True)  # для стабильности\n",
    "    return np.exp(X - x_max) / np.sum(np.exp(X - x_max), axis=-1, keepdims=True)\n",
    "\n",
    "def self_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Q.shape = (seq_len, d_model)\n",
    "    K.shape = (seq_len, d_model)\n",
    "    V.shape = (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    d_model = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_model)\n",
    "    if mask is not None:\n",
    "        scores += mask\n",
    "    soft_scores = softmax(scores)\n",
    "    scores = np.matmul(soft_scores, V)\n",
    "    return scores\n",
    "\n",
    "def compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute Query (Q), Key (K), and Value (V) matrices.\n",
    "    \"\"\"\n",
    "    return np.dot(X, W_q), np.dot(X, W_k), np.dot(X, W_v)\n",
    "\n",
    "def masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute masked self-attention.\n",
    "    \"\"\"\n",
    "    attn = self_attention(Q, K, V, mask)\n",
    "    return attn\n",
    "\n",
    "print(masked_attention(Q, K, V, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  3.,  4.,  9., 12., 16.],\n",
       "       [ 1.,  5.,  6., 25., 30., 36.]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Polynomial Features\n",
    "\n",
    "X = np.array([[2, 3],\n",
    "            [3, 4],\n",
    "            [5, 6]])\n",
    "degree = 2\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def polynomial_features(X, degree):\n",
    "    if X.ndim == 1:\n",
    "        X = X[:, np.newaxis]\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    comb = [combinations_with_replacement(range(n_features), d) for d in range(1, degree + 1)]\n",
    "    comb = [item for sublist in comb for item in sublist]\n",
    "    \n",
    "    poly_features = [np.prod(X[:, c], axis=1) for c in comb]\n",
    "    poly_features = np.column_stack(poly_features)\n",
    "    \n",
    "    return np.hstack([np.ones((n_samples, 1)), poly_features])\n",
    "\n",
    "polynomial_features(X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.693, 0.   , 0.   ])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BM25 Ranking\n",
    "\n",
    "corpus = [['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']]\n",
    "query = ['the', 'cat']\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def compute_term_freq(document, term):\n",
    "    return Counter(document).get(term, 0)\n",
    "\n",
    "def avdl(corpus):\n",
    "    total_words = sum(len(doc) for doc in corpus)\n",
    "    return total_words / len(corpus)\n",
    "\n",
    "def compute_df_term(corpus):\n",
    "    doc_freqs = Counter()\n",
    "    for doc in corpus:\n",
    "        doc_freqs.update(set(doc))\n",
    "    return doc_freqs\n",
    "\n",
    "def compute_idf_term(N, df):\n",
    "    return np.log((N + 1) / (df + 1))\n",
    "\n",
    "def calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n",
    "    if not corpus or not query:\n",
    "        raise ValueError(\"Corpus and query cannot be empty\")\n",
    "    \n",
    "    N = len(corpus)\n",
    "    avg_doc_len = avdl(corpus)\n",
    "    doc_freqs = compute_df_term(corpus)\n",
    "    idf_cache = {q: compute_idf_term(N, doc_freqs.get(q, 0)) for q in query}\n",
    "    \n",
    "    scores = []\n",
    "    for doc in corpus:\n",
    "        doc_len = len(doc)\n",
    "        term_freq = Counter(doc)\n",
    "        doc_score = 0\n",
    "        for q in query:\n",
    "            if q in term_freq:\n",
    "                tf = term_freq[q]\n",
    "                idf = idf_cache[q]\n",
    "                doc_len_norm = 1 - b + b * (doc_len / avg_doc_len)\n",
    "                term_score = (tf * (k1 + 1)) / (tf + k1 * doc_len_norm)\n",
    "                doc_score += idf * term_score\n",
    "        scores.append(doc_score)\n",
    "    \n",
    "    return np.round(scores, 3)\n",
    "\n",
    "calculate_bm25_scores(corpus, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 Text Generation\n",
    "\n",
    "prompt=\"hello\"\n",
    "n_tokens_to_generate=5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n",
    "    x = wte[inputs] + wpe[range(len(inputs))] \n",
    "    # for block in blocks:\n",
    "    #     x = transformer_block(x, **block, n_head=n_head)\n",
    "    return layer_norm(x, **ln_f) @ wte.T\n",
    "\n",
    "def layer_norm(x, g, b, eps=1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    return g * (x - mean) / np.sqrt(variance + eps) + b\n",
    "\n",
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    for _ in range(n_tokens_to_generate):\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)\n",
    "        next_id = np.argmax(logits[-1])\n",
    "        inputs.append(int(next_id))\n",
    "    return inputs[len(inputs) - n_tokens_to_generate:]\n",
    "\n",
    "def gen_text(prompt: str, n_tokens_to_generate: int = 40):\n",
    "    np.random.seed(42)\n",
    "    encoder, hparams, params = load_encoder_hparams_and_params()\n",
    "    input_ids = encoder.encode(prompt)\n",
    "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "    output_text = encoder.decode(output_ids)\n",
    "    return output_text\n",
    "\n",
    "def load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n",
    "    class DummyBPE:\n",
    "        def __init__(self):\n",
    "            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n",
    "\n",
    "        def encode(self, text: str):\n",
    "            tokens = text.strip().split()\n",
    "            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "        def decode(self, token_ids: list):\n",
    "            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n",
    "            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n",
    "\n",
    "    hparams = {\n",
    "        \"n_ctx\": 1024,\n",
    "        \"n_head\": 12\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"wte\": np.random.rand(3, 10),\n",
    "        \"wpe\": np.random.rand(1024, 10),\n",
    "        \"blocks\": [],\n",
    "        \"ln_f\": {\n",
    "            \"g\": np.ones(10),\n",
    "            \"b\": np.zeros(10),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    encoder = DummyBPE()\n",
    "    return encoder, hparams, params\n",
    "\n",
    "\n",
    "gen_text('hello world', n_tokens_to_generate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это полностью процесс как в gpt2\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def layer_norm(x, g, b, eps=1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    return g * (x - mean) / np.sqrt(variance + eps) + b\n",
    "\n",
    "def linear(x, w, b):\n",
    "    return x @ w + b\n",
    "\n",
    "def ffn(x, c_fc, c_proj):\n",
    "    return linear(gelu(linear(x, **c_fc)), **c_proj)\n",
    "\n",
    "def attention(q, k, v, mask):\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
    "\n",
    "def mha(x, c_attn, c_proj, n_head):\n",
    "    x = linear(x, **c_attn)\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), np.split(x, 3, axis=-1)))\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n",
    "    x = linear(np.hstack(out_heads), **c_proj)\n",
    "    return x\n",
    "\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n",
    "    return x\n",
    "\n",
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n",
    "    x = wte[inputs] + wpe[range(len(inputs))] # position embeddings\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block, n_head=n_head)\n",
    "    return layer_norm(x, **ln_f) @ wte.T\n",
    "\n",
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    for _ in range(n_tokens_to_generate):\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)\n",
    "        next_id = np.argmax(logits[-1])\n",
    "        inputs.append(int(next_id))\n",
    "    return inputs[len(inputs) - n_tokens_to_generate:]\n",
    "\n",
    "def gen_text(prompt: str, n_tokens_to_generate: int = 40):\n",
    "    np.random.seed(42)  # Set the random seed for reproducibility\n",
    "    encoder, hparams, params = load_encoder_hparams_and_params()\n",
    "    input_ids = encoder.encode(prompt)\n",
    "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "    output_text = encoder.decode(output_ids)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\n"
     ]
    }
   ],
   "source": [
    "# Calculate Performance Metrics for a Classification Model\n",
    "\n",
    "actual = [1, 0, 1, 0, 1]\n",
    "predicted = [1, 0, 0, 1, 1]\n",
    "\n",
    "def performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for ind in range(len(actual)):\n",
    "        if actual[ind] == 0 and predicted[ind] == 0:\n",
    "            tn += 1\n",
    "        elif actual[ind] == 1 and predicted[ind] == 1:\n",
    "            tp += 1\n",
    "        elif actual[ind] == 1 and predicted[ind] == 0:\n",
    "            fn += 1\n",
    "        elif actual[ind] == 0 and predicted[ind] == 1:\n",
    "            fp += 1\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fn + fp + tn)\n",
    "    \n",
    "    confusion_matrix = [[tp, fn], [fp, tn]]\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    specificity = tn / (tn + fp)\n",
    "    negativePredictive = tn / (tn + fn)\n",
    "\n",
    "    return confusion_matrix, round(accuracy, 3), round(f1, 3), round(specificity, 3), round(negativePredictive, 3)\n",
    "\n",
    "print(performance_metrics(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Layer Normalization for Sequence Data\n",
    "\n",
    "np.random.seed(42); \n",
    "X = np.random.randn(2, 2, 3);\n",
    "gamma = np.ones(3).reshape(1, 1, -1);\n",
    "beta = np.zeros(3).reshape(1, 1, -1);\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n",
    "\t\"\"\"\n",
    "\tPerform Layer Normalization.\n",
    "\t\"\"\"\n",
    "\tmean = np.mean(X, axis=-1, keepdims=True)\n",
    "\tvariance = np.var(X, axis=-1, keepdims=True)\n",
    "\treturn gamma * (X - mean) / np.sqrt(variance + epsilon) + beta\n",
    "\n",
    "\n",
    "layer_normalization(X, gamma, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.0097, 0.0286, 0.015, 0.0135, 0.0316],\n",
       " [6.9315,\n",
       "  6.9075,\n",
       "  6.8837,\n",
       "  6.8601,\n",
       "  6.8367,\n",
       "  6.8134,\n",
       "  6.7904,\n",
       "  6.7675,\n",
       "  6.7448,\n",
       "  6.7223])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Logistic Regression with Gradient Descent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def bce(y_true, y_pred):\n",
    "    loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "def train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n",
    "    \"\"\"\n",
    "    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n",
    "    \"\"\"\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X)) # добавляем баес к весам - для этого добавляем в X столбик с 1 - в начало\n",
    "    y = y.reshape(-1, 1)\n",
    "    B = np.zeros((X.shape[1], 1)) # тогда нужно считать всего одну матрицу\n",
    "\n",
    "    losses = []\n",
    "    for _ in range(iterations):\n",
    "        z = X @ B\n",
    "        y_pred = sigmoid(z) \n",
    "        dl_db = X.T @ (y_pred - y) # производная bce loss\n",
    "        B -= learning_rate * dl_db\n",
    "        loss = bce(y, y_pred)\n",
    "        losses.append(round(loss.item(), 4))\n",
    "\n",
    "    return B.flatten().round(4).tolist(), losses\n",
    "\n",
    "\n",
    "X = np.array([[ 0.76743473, -0.23413696, -0.23415337,  1.57921282],\n",
    "    [-1.4123037 ,  0.31424733, -1.01283112, -0.90802408],\n",
    "    [-0.46572975,  0.54256004, -0.46947439, -0.46341769],\n",
    "    [-0.56228753, -1.91328024,  0.24196227, -1.72491783],\n",
    "    [-1.42474819, -0.2257763 ,  1.46564877,  0.0675282 ],\n",
    "    [ 1.85227818, -0.29169375, -0.60063869, -0.60170661],\n",
    "    [ 0.37569802,  0.11092259, -0.54438272, -1.15099358],\n",
    "    [ 0.19686124, -1.95967012,  0.2088636 , -1.32818605],\n",
    "    [ 1.52302986, -0.1382643 ,  0.49671415,  0.64768854],\n",
    "    [-1.22084365, -1.05771093, -0.01349722,  0.82254491]])\n",
    "y = np.array([1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
    "learning_rate = 1e-3\n",
    "iterations = 10\n",
    "train_logreg(X, y, learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Softmax Regression with Gradient Descent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "def train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n",
    "    \"\"\"\n",
    "    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n",
    "    \"\"\"\n",
    "    n_classes = len(np.unique(y))\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))  # Теперь X имеет размер (m, n + 1).\n",
    "    y_one_hot = np.eye(n_classes)[y] # Преобразуем вектор меток y в one-hot представление для работы с Softmax\n",
    "    B = np.zeros((X.shape[1], n_classes)) \n",
    "\n",
    "    losses = []\n",
    "    for _ in range(iterations):\n",
    "        z = X @ B\n",
    "        y_pred = softmax(z)   (X.shape[0], n_classes)\n",
    "        print(y_pred.shape)\n",
    "        \n",
    "        dl_db = (X.T @ (y_pred - y_one_hot)) # \n",
    "        B -= learning_rate * dl_db\n",
    "        loss = cross_entropy(y_one_hot, y_pred)\n",
    "        losses.append(round(loss.item(), 4))\n",
    "\n",
    "    return np.round(B.T, 4).tolist(), losses\n",
    "\n",
    "print(train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], \n",
    "                                [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], \n",
    "                                [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], \n",
    "                                [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], \n",
    "                                [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], \n",
    "                                [0.5482, 0.3315, 0.1067, 0.3069, -0.3755],\n",
    "                                [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], \n",
    "                                [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], \n",
    "                                [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], \n",
    "                                [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# solve 4x^2 - 5x + 1.5 = 0 with gradient descent\n",
    "# 1. возводим в квадрат, потому что мы должны найти градиент - (4x^2 - 5x + 1.5)^2 = 0\n",
    "# 2. derivative: 2 * (4x^2 − 5x + 1.5) * (8x − 5)\n",
    "\n",
    "def grad(x):\n",
    "    return 2 * (4 * x * x - 5 * x + 1.5) * (8 * x - 5)\n",
    "\n",
    "def solve(iterations: int = 1000, eps: float = 1e-2, learning_rate: float = 1e-2):\n",
    "    x0 = 0.0\n",
    "    it = 0\n",
    "    xs = []\n",
    "    while it < iterations:\n",
    "        if abs(grad(x0)) < eps:\n",
    "            break\n",
    "        x0 -= learning_rate * grad(x0)\n",
    "        it += 1\n",
    "        xs.append(x0)\n",
    "    return xs\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a, b, c = 4, -5, 1.5\n",
    "\n",
    "x = np.linspace(-1, 2, 500)\n",
    "y = a * x**2 + b * x + c\n",
    "\n",
    "roots = np.roots([a, b, c])\n",
    "solution_x_0 = roots[0]\n",
    "solution_x_1 = roots[1]\n",
    "solution_y_0 = a * solution_x_0**2 + b * solution_x_0 + c\n",
    "\n",
    "# Массив точек p (пример) - только координаты x\n",
    "p = np.array(solve())\n",
    "p_y = a * p**2 + b * p + c\n",
    "\n",
    "# Построение графика\n",
    "plt.plot(x, y, label='4x² - 5x + 1.5', color='royalblue')\n",
    "plt.scatter(p, p_y, color='crimson', label='Точки p', zorder=3)\n",
    "\n",
    "# Добавление перпендикулярной линии к решению уравнения\n",
    "plt.axvline(solution_x_0, color='green', linestyle='--', label=f'Решение x={solution_x_0:.2f}')\n",
    "plt.axvline(solution_x_1, color='green', linestyle='--', label=f'Решение x={solution_x_1:.2f}')\n",
    "# Добавление сетки, легенды и заголовков\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.axhline(0, color='black', lw=0.8)\n",
    "plt.axvline(0, color='black', lw=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('График функции 4x² - 5x + 1.5 и точки p')\n",
    "\n",
    "# Показать график\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
