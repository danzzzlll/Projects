{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is some exercises from [deep-ml](https://www.deep-ml.com/problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"it is sunny morning, when i want to go to school\",\n",
    "    \"when it is dead you cant create something new что\",\n",
    "    \"что такое осень, это небо, небо и слякоть под ногами it has been go\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def tf(doc, uniq_words):\n",
    "    \"\"\"Compute term frequency for a single document\"\"\"\n",
    "    word_count = Counter(doc.split())\n",
    "    total_words = len(doc.split())  \n",
    "    return np.array([word_count[word] / total_words if total_words > 0 else 0 for word in uniq_words])\n",
    "\n",
    "def idf(docs, uniq_words):\n",
    "    \"\"\"Compute inverse document frequency across all documents\"\"\"\n",
    "    n = len(docs)\n",
    "    df = np.array([sum(1 for doc in docs if word in doc.split()) for word in uniq_words], dtype=np.float64)\n",
    "    idf_m = np.log(n / (df + 1e-6))  \n",
    "    return idf_m\n",
    "\n",
    "def tfidf(docs, word=None):\n",
    "    \"\"\"Compute the TF-IDF matrix for all documents\"\"\"\n",
    "    uniq_words = list(set(word for doc in docs for word in doc.split()))\n",
    "    idf_values = idf(docs, uniq_words)\n",
    "    tfidf_matrix = np.array([tf(doc, uniq_words) * idf_values for doc in docs])\n",
    "    return tfidf_matrix, uniq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def tf(doc, uniq_words):\n",
    "    \"\"\"Compute term frequency for a single document\"\"\"\n",
    "    word_count = Counter(doc)\n",
    "    total_words = len(doc)  \n",
    "    return np.array([word_count[word] / total_words if total_words > 0 else 0 for word in uniq_words])\n",
    "\n",
    "def idf(docs, uniq_words):\n",
    "    \"\"\"Compute inverse document frequency across all documents\"\"\"\n",
    "    n = len(docs)\n",
    "    df = np.array([sum(1 for doc in docs if word in doc) for word in uniq_words], dtype=np.float64)\n",
    "    idf_m = np.log(n / (df + 1))  \n",
    "    return idf_m\n",
    "\n",
    "def compute_tf_idf(corpus, query):\n",
    "\t\"\"\"\n",
    "\tCompute TF-IDF scores for a query against a corpus of documents.\n",
    "    \n",
    "\t:param corpus: List of documents, where each document is a list of words\n",
    "\t:param query: List of words in the query\n",
    "\t:return: List of lists containing TF-IDF scores for the query words in each document\n",
    "\t\"\"\"\n",
    "\tuniq_words = list(set(word for doc in corpus for word in doc))\n",
    "\tidf_values = idf(corpus, uniq_words)\n",
    "\ttfidf_matrix = np.array([tf(doc, uniq_words) * idf_values for doc in corpus])\n",
    "\tprint(uniq_words)\n",
    "\tindexes = [uniq_words.index(q) for q in query]\n",
    "\tprint(indexes)\n",
    "\treturn tfidf_matrix, [[elem] for elem in tfidf_matrix[:, indexes]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n",
    "    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n",
    "]\n",
    "query = [\"cat\"]\n",
    "\n",
    "print(compute_tf_idf(corpus, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix-Vector Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2], [2, 4]]\n",
    "b = [1, 2]\n",
    "\n",
    "def matrix_dot_vector(a, b):\n",
    "    if len(a[0]) != len(b):\n",
    "        return -1\n",
    "    return [sum(row[i] * b[i] for i in range(len(b))) for row in a]\n",
    "\n",
    "def matrix_dot_vector(a, b):\n",
    "    # Return a list where each element is the dot product of a row of 'a' with 'b'.\n",
    "    # If the number of columns in 'a' does not match the length of 'b', return -1.\n",
    "    if len(a[0]) != len(b):\n",
    "        return -1\n",
    "    answer = []\n",
    "    s = 0\n",
    "    for row in a:\n",
    "        for ind, elem in enumerate(row):\n",
    "            s += elem * b[ind]\n",
    "        answer.append(s)\n",
    "        s = 0\n",
    "        \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = [[1,2,3,4],[5,6,7,8]]\n",
    "new_shape = (4, 2)\n",
    "\n",
    "def reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n",
    "    #Write your code here and return a python list after reshaping by using numpy's tolist() method \n",
    "    try:\n",
    "        return np.array(a).reshape(new_shape).tolist()\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "reshape_matrix(a, new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean by Row or Column\n",
    "\n",
    "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "mode = 'column'\n",
    "\n",
    "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n",
    "    if mode == 'row':\n",
    "        return [\n",
    "            sum(row) / len(row) for row in matrix\n",
    "        ]\n",
    "    # s = 0\n",
    "    # answer = []\n",
    "    # for ind, elem in enumerate(matrix):\n",
    "    #     for row in matrix:\n",
    "    #         s += row[ind]\n",
    "    #     answer.append(s / len(matrix))\n",
    "    #     s = 0\n",
    "    # return answer\n",
    "    return [sum(row[i] for row in matrix) / len(matrix) for i in range(len(matrix[0]))]\n",
    "\n",
    "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "calculate_matrix_mean(matrix, 'column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar Multiplication of a Matrix\n",
    "\n",
    "matrix = [[1, 2, 3], [3, 4]]\n",
    "scalar = 2\n",
    "\n",
    "def scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n",
    "\n",
    "\treturn\t[\n",
    "\t\t\t[matrix[j][i] * scalar for i in range(len(matrix))]\n",
    "\t\t\tfor j in range(len(matrix))\n",
    "        ]\n",
    "\n",
    "scalar_multiply(matrix, scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Eigenvalues of a Matrix\n",
    "\n",
    "matrix = [[2, 1], [1, 2]]\n",
    "import math\n",
    "def calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n",
    "    a = matrix[0][0]\n",
    "    b = matrix[0][1]\n",
    "    c = matrix[1][0]\n",
    "    d = matrix[1][1]\n",
    "\n",
    "    tr = a + d\n",
    "    det = a * d - b * c\n",
    "\n",
    "    descrim = tr * tr - 4 * 1 * det\n",
    "\n",
    "    x1 = (tr + math.sqrt(descrim)) / 2 * 1\n",
    "    x2 = (tr - math.sqrt(descrim)) / 2 * 1\n",
    "\n",
    "    return sorted([x1, x2], reverse=True)\n",
    "\n",
    "calculate_eigenvalues(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Using Normal Equation\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([1, 2, 3])\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    y = y.reshape(-1, 1)\n",
    "    for _ in range(iterations):\n",
    "        y_pred = X @ theta\n",
    "        error = y_pred - y\n",
    "        theta -= grad(m, X, error) * alpha\n",
    "\n",
    "    return np.round(theta, 4)\n",
    "\n",
    "def grad(m, X, error):\n",
    "    return (1 / m) * (X.T @ error)\n",
    "\n",
    "linear_regression_gradient_descent(X, y, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Implement TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n",
    "    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n",
    "]\n",
    "query = [\"cat\"]\n",
    "\n",
    "def compute_tf(doc, uniq_words):\n",
    "    \"\"\"Compute term frequency for a document\"\"\"\n",
    "    word_count = {word: 0 for word in uniq_words}\n",
    "    total_words = len(doc)\n",
    "\n",
    "    for word in doc:\n",
    "        word_count[word] += 1\n",
    "    \n",
    "    return {word: word_count[word] / total_words if total_words > 0 else 0 for word in uniq_words}\n",
    "\n",
    "\n",
    "def compute_idf(corpus, uniq_words):\n",
    "    \"\"\"Compute inverse document frequency for all words in the corpus\"\"\"\n",
    "    num_docs = len(corpus)\n",
    "    doc_freq = {word: 0 for word in uniq_words}\n",
    "\n",
    "    for word in uniq_words:\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                doc_freq[word] += 1\n",
    "    \n",
    "    return {word: math.log((num_docs + 1) / (doc_freq[word] + 1)) + 1 for word in uniq_words}\n",
    "\t\n",
    "\n",
    "def compute_tf_idf(corpus, query):\n",
    "\t\"\"\"\n",
    "\tCompute TF-IDF scores for a query against a corpus of documents.\n",
    "    \n",
    "\t:param corpus: List of documents, where each document is a list of words\n",
    "\t:param query: List of words in the query\n",
    "\t:return: List of lists containing TF-IDF scores for the query words in each document\n",
    "\t\"\"\"\n",
    "\tif not corpus:\n",
    "\t\treturn []\n",
    "\tuniq_words = list(\n",
    "\t\tset(\n",
    "\t\t\t[\n",
    "\t\t\t\tword for doc in corpus\n",
    "\t\t\t\tfor word in doc\n",
    "\t\t\t]\n",
    "\t\t))\n",
    "      \n",
    "\tidf_values = compute_idf(corpus, uniq_words)\n",
    "\t\n",
    "\ttfidf_matrix = []\n",
    "\tfor doc in corpus:\n",
    "\t\ttf = compute_tf(doc=doc, uniq_words=uniq_words)\n",
    "\t\ttfidf_scores = [tf.get(q, 0) * idf_values.get(q, 0) for q in query]\n",
    "\t\ttfidf_matrix.append(tfidf_scores)\n",
    "\treturn np.round(tfidf_matrix, 5).tolist()\n",
    "\t\n",
    "compute_tf_idf(corpus, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Neuron\n",
    "\n",
    "features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]]\n",
    "labels = [0, 1, 0]\n",
    "weights = [0.7, -0.4]\n",
    "bias = -0.1\n",
    "\n",
    "import math\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "def single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n",
    "\n",
    "    out_matrix = [sum(row[j] * weights[j] for j in range(len(weights))) for row in features]\n",
    "    bias_matrix = [elem + bias for elem in out_matrix]\n",
    "    probabilities = [round(sigmoid(elem), 4) for elem in bias_matrix]\n",
    "    mse = round(sum([math.pow((prob - true), 2) for prob, true in zip(probabilities,labels )]) / len(labels), 4)\n",
    "    return probabilities, mse\n",
    "\n",
    "single_neuron_model(features, labels, weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single neron with backprop\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int):\n",
    "    weights = np.array(initial_weights, dtype=np.float64)\n",
    "    bias = float(initial_bias)\n",
    "    m = len(labels)  # Number of samples\n",
    "    mse_values = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Compute predictions\n",
    "        linear_output = np.dot(features, weights) + bias\n",
    "        predictions = sigmoid(linear_output)\n",
    "\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        errors = predictions - labels\n",
    "        mse = np.mean(errors ** 2)\n",
    "        mse_values.append(round(mse, 4).item())\n",
    "\n",
    "        # Compute gradients\n",
    "        gradient_weights = (2 / m) * np.dot(features.T, errors * predictions * (1 - predictions))\n",
    "        gradient_bias = (2 / m) * np.sum(errors * predictions * (1 - predictions))\n",
    "\n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * gradient_weights\n",
    "        bias -= learning_rate * gradient_bias\n",
    "\n",
    "    return np.round(weights, 4).tolist(), round(bias, 4).item(), mse_values\n",
    "\n",
    "# Test Case\n",
    "print(train_neuron(\n",
    "    np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]),\n",
    "    np.array([1, 0, 0]),\n",
    "    np.array([0.1, -0.2]),\n",
    "    0.0,\n",
    "    0.1,\n",
    "    100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log softmax\n",
    "\n",
    "A = np.array([1, 2, 3])\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def log(scores):\n",
    "\treturn math.log(sum([\n",
    "\t\t\tmath.exp(x - max(scores)) for x in scores\n",
    "\t\t]))\n",
    "\n",
    "def log_softmax(scores: list) -> np.ndarray:\n",
    "\treturn np.array([\n",
    "\t\tround((x - max(scores) - log(scores)), 4)\n",
    "\t\tfor x in scores\n",
    "\t])\n",
    "\n",
    "\n",
    "log_softmax(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Basic Autograd Operations\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\t\t\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        out = Value(data=(self.data + other.data), _children=(self, other), _op=\"+\")\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        out = Value(data=(self.data * other.data), _children=(self, other), _op=\"*\")\n",
    "        def _backward():\n",
    "            self.grad += (other.data * out.grad)\n",
    "            other.grad += (self.data * out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(data=max(0, self.data), _children=(self,), _op=\"relu\")\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\t\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "a = Value(2)\n",
    "b = Value(5)\n",
    "c = Value(10)\n",
    "d = a + b * c\n",
    "e = d.relu()\n",
    "e.backward()\n",
    "print(a, b, c, d, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Pattern Weaver's Code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n = 5\n",
    "crystal_values = [4, 2, 7, 1, 9]\n",
    "dimension = 1\n",
    "\n",
    "def softmax(values):\n",
    "    exp_values = np.exp(values - np.max(values)) \n",
    "    return exp_values / np.sum(exp_values, axis=0)\n",
    "\n",
    "def pattern_weaver(n, crystal_values, dimension):\n",
    "    crystal_values = np.array(crystal_values).reshape(n, dimension)\n",
    "    attention_scores = crystal_values @ crystal_values.T \n",
    "    attention_weights = np.apply_along_axis(softmax, 1, attention_scores)\n",
    "    weighted_patterns = attention_weights @ crystal_values\n",
    "    return np.round(weighted_patterns.flatten(), 4).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Adam Optimization Algorithm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n",
    "    m = 0\n",
    "    v = 0\n",
    "    for it in range(num_iterations):\n",
    "        it += 1\n",
    "        gradient = grad(x0)\n",
    "        \n",
    "        m = beta1 * m + (1 - beta1) * gradient\n",
    "        v = beta2 * v + (1 - beta2) * gradient * gradient\n",
    "\n",
    "        m_up = m / (1 - beta1 ** it)    \n",
    "        v_up = v / (1 - beta2 ** it)\n",
    "\n",
    "        x0 = x0 - learning_rate * m_up / (np.sqrt(v_up) + epsilon)\n",
    "        \n",
    "    return x0\n",
    "\n",
    "def objective_function(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient(x):\n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "\n",
    "x0 = np.array([0.2, 12.3])\n",
    "x_opt = adam_optimizer(objective_function, gradient, x0)\n",
    "\n",
    "print(\"Optimized parameters:\", x_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using the Adam optimizer.\n",
    "    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n",
    "    :param parameter: Current parameter value\n",
    "    :param grad: Current gradient\n",
    "    :param m: First moment estimate\n",
    "    :param v: Second moment estimate\n",
    "    :param t: Current timestep\n",
    "    :param learning_rate: Learning rate (default=0.001)\n",
    "    :param beta1: First moment decay rate (default=0.9)\n",
    "    :param beta2: Second moment decay rate (default=0.999)\n",
    "    :param epsilon: Small constant for numerical stability (default=1e-8)\n",
    "    :return: tuple: (updated_parameter, updated_m, updated_v)\n",
    "    \"\"\"\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * grad * grad\n",
    "\n",
    "    m_up = m / (1 - beta1 ** t)    \n",
    "    v_up = v / (1 - beta2 ** t)\n",
    "\n",
    "    x0 = learning_rate * m_up / (np.sqrt(v_up) + epsilon)\n",
    "    parameter = parameter - x0\n",
    "\n",
    "    # Your code here\n",
    "    return np.round(parameter,5), np.round(m,5), np.round(v,5)\n",
    "\n",
    "\n",
    "parameter = 1.0\n",
    "grad = 0.1\n",
    "m = 0.0\n",
    "v = 0.0\n",
    "t = 1\n",
    "\n",
    "adam_optimizer(parameter, grad, m, v, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class Layer(object):\n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape\n",
    "\n",
    "    def layer_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def parameters(self):\n",
    "        return 0\n",
    "\n",
    "    def forward_pass(self, X, training):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def output_shape(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, n_units, input_shape=None):\n",
    "        self.layer_input = None\n",
    "        self.n_units = n_units\n",
    "        self.trainable = True\n",
    "        self.W = None\n",
    "        self.w0 = None\n",
    "        self.input_shape = input_shape  # Optional\n",
    "\n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape  # Ensure input shape is set before initialization\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        if self.input_shape is None:\n",
    "            raise ValueError(\"Input shape must be set before initializing the layer.\")\n",
    "\n",
    "        limit = 1 / math.sqrt(self.input_shape[0])\n",
    "        self.W = np.random.uniform(low=-limit, high=limit, size=(self.input_shape[0], self.n_units))\n",
    "        self.w0 = np.zeros(shape=(1, self.n_units))\n",
    "\n",
    "        # Assign optimizer without copying\n",
    "        self.W_opt = optimizer\n",
    "        self.w0_opt = optimizer\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.input_shape[0] * self.n_units + self.n_units\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "        return X.dot(self.W) + self.w0 \n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        if self.trainable:\n",
    "            grad_W = self.layer_input.T.dot(accum_grad)\n",
    "            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n",
    "\n",
    "            self.W = self.W_opt.update(self.W, grad_W)\n",
    "            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n",
    "\n",
    "        return accum_grad.dot(self.W.T)\n",
    "\n",
    "class MockOptimizer:\n",
    "    def update(self, weights, grad):\n",
    "        return weights - 0.01 * grad    \n",
    "\n",
    "optimizer = MockOptimizer()\n",
    "\n",
    "dense_layer = Dense(n_units=3, input_shape=(2,))\n",
    "dense_layer.initialize(optimizer)\n",
    "\n",
    "X = np.array([[1, 2]])\n",
    "output = dense_layer.forward_pass(X)\n",
    "print(\"Forward pass output:\", output)\n",
    "\n",
    "accum_grad = np.array([[0.1, 0.2, 0.3]])\n",
    "back_output = dense_layer.backward_pass(accum_grad)\n",
    "print(\"Backward pass output:\", back_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention formula\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    return X.dot(W_q), X.dot(W_k), X.dot(W_v)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "\n",
    "    attn_score = Q.dot(K.T) / np.sqrt(K.shape[0])\n",
    "\n",
    "    sft = softmax(attn_score)\n",
    "\n",
    "    attn_output = sft.dot(V)\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PReLU\n",
    "\n",
    "def prelu(x: float, alpha: float = 0.25):\n",
    "    return (x if x > 0 else alpha * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def softplus(x: float):\n",
    "    return round(math.log(1 + math.exp(x)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product Calculator\n",
    "\n",
    "vec1 = np.array([1, 2, 3])\n",
    "vec2 = np.array([4, 5, 6])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_dot_product(vec1, vec2) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the dot product of two vectors.\n",
    "    Args:\n",
    "        vec1 (numpy.ndarray): 1D array representing the first vector.\n",
    "        vec2 (numpy.ndarray): 1D array representing the second vector.\n",
    "    \"\"\"\n",
    "    return np.sum(vec1 * vec2).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cosine Similarity Between Vectors\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.sum(v1 * v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Gradient Descent Variants with MSE Loss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "batch_size = 2\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.zeros(X.shape[1])\n",
    "\n",
    "def grad(X, error):\n",
    "    return 2 / X.shape[0] * (X.T @ error)\n",
    "\n",
    "def grad_one(x, error):\n",
    "    return 2 * error * x\n",
    "\n",
    "def batchify(X, y, batch_size):\n",
    "    indices = np.random.permutation(len(y))\n",
    "    X, y = X[indices], y[indices]\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        yield X[i:i + batch_size], y[i:i + batch_size]\n",
    "\n",
    "def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n",
    "    for i in range(n_iterations):\n",
    "        if method == \"batch\":\n",
    "            y_pred = X.dot(weights)\n",
    "            error = y_pred - y\n",
    "            weights = weights - learning_rate * grad(X, error)\n",
    "        elif method == \"stochastic\":\n",
    "            for example, target in zip(X, y):\n",
    "                y_pred = example.dot(weights)\n",
    "                error = y_pred - target\n",
    "                weights = weights - learning_rate * grad_one(example, error)\n",
    "        elif method == \"mini_batch\":\n",
    "            generator = batchify(X, y, batch_size)\n",
    "            for x_, y_ in generator:\n",
    "                y_pred = x_.dot(weights)\n",
    "                error = y_pred - y_\n",
    "                weights = weights - learning_rate * grad(x_, error)\n",
    "        if i % 100 == 0: \n",
    "            print(f\"Iteration {i}, Weights: {weights}\")\n",
    "    return weights\n",
    "\n",
    "# Test Batch Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='mini_batch')\n",
    "print(final_weights)\n",
    "# Test Stochastic Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n",
    "print(final_weights)\n",
    "# # Test Mini-Batch Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n",
    "print(final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Lasso Regression using Gradient Descent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [1, 1], [2, 2]])\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "def l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        y_pred = X @ weights + bias\n",
    "        error = y_pred - y\n",
    "\n",
    "        grad_w = X.T.dot(error) / n_samples + alpha * np.sign(weights)\n",
    "        grad_b = np.sum(error) / n_samples\n",
    "\n",
    "        weights = weights - learning_rate * grad_w\n",
    "        bias = bias - learning_rate * grad_b\n",
    "\n",
    "        weights[np.abs(weights) < tol] = 0 \n",
    "\n",
    "        if np.linalg.norm(grad_w, ord=1) < tol:\n",
    "            break \n",
    "\n",
    "        # print(error, weights, bias)\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "weights, bias = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\n",
    "\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Implementing a Simple RNN\n",
    "Write a Python function that implements a simple Recurrent Neural Network (RNN) cell. The function should process a sequence of input vectors and produce the final hidden state. Use the tanh activation function for the hidden state updates. The function should take as inputs the sequence of input vectors, the initial hidden state, the weight matrices for input-to-hidden and hidden-to-hidden connections, and the bias vector. The function should return the final hidden state after processing the entire sequence, rounded to four decimal places.\n",
    "\"\"\"\n",
    "\n",
    "input_sequence = [[1.0], [2.0], [3.0]]\n",
    "initial_hidden_state = [0.0]\n",
    "Wx = [[0.5]]  # Input to hidden weights\n",
    "Wh = [[0.8]]  # Hidden to hidden weights\n",
    "b = [0.0]     # Bias\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n",
    "    input_sequence = np.array(input_sequence)\n",
    "    initial_hidden_state = np.array(initial_hidden_state)\n",
    "    h = initial_hidden_state.copy()\n",
    "    for x in input_sequence:\n",
    "        # print(Wx, x, Wx @ x, Wh, h, Wh @ h, b)\n",
    "        h = np.tanh(Wx @ x + Wh @ h + b)\n",
    "        # print(h)\n",
    "    final_hidden_state = h.copy()\n",
    "    return final_hidden_state\n",
    "\n",
    "rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a Simple RNN with Backpropagation Through Time (BPTT)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\n",
    "expected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n",
    "\n",
    "def grad(y_pred, y):\n",
    "    return y_pred - y\n",
    "\n",
    "def mse(y_pred, y):\n",
    "    return np.mean(np.pow(y_pred - y, 2))\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the RNN with random weights and zero biases.\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN for a given sequence of inputs.\n",
    "        \"\"\"\n",
    "        timestamps, input_size = x.shape\n",
    "        cache = {\n",
    "            \"hidden_states\": [],\n",
    "            \"y_pred\": []\n",
    "        }\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for timestamp in range(timestamps):\n",
    "            h = np.tanh(self.W_xh @ x[timestamp].reshape(-1, 1) + self.W_hh @ h + self.b_h)\n",
    "            y = self.W_hy @ h + self.b_y\n",
    "            cache[\"hidden_states\"].append(h.copy())\n",
    "            cache[\"y_pred\"].append(y.copy())\n",
    "        self.cache = cache.copy()\n",
    "        return cache\n",
    "\n",
    "\n",
    "    def backward(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Backpropagation through time to adjust weights based on error gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize empty fields\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "        h_ts = self.cache[\"hidden_states\"]\n",
    "        y_ts = self.cache[\"y_pred\"]\n",
    "        timestamps, input_size = x.shape\n",
    "        loss = 0.0\n",
    "        full_loss = []\n",
    "        for t in reversed(range(timestamps)):\n",
    "            dL_dy = grad(y_ts[t], y[t].reshape(-1, 1))\n",
    "            loss += mse(y_ts[t], y[t].reshape(-1, 1))\n",
    "            dW_hy += dL_dy @ h_ts[t].T\n",
    "            db_y += dL_dy\n",
    "            dL_dh = self.W_hy.T @ dL_dy + dh_next\n",
    "\n",
    "            dtanh = (1 - self.cache[\"hidden_states\"][t] ** 2) * dL_dh\n",
    "\n",
    "            dW_hh += dtanh @ h_ts[t - 1].T\n",
    "            dW_xh += dtanh @ x[t].reshape(-1, 1)\n",
    "            db_h += dtanh\n",
    "            dh_next = self.W_hh.T @ dtanh\n",
    "        full_loss.append(loss / timestamps)\n",
    "        print(loss / timestamps)\n",
    "        \n",
    "        self.W_xh -= learning_rate * dW_xh\n",
    "        self.W_hh -= learning_rate * dW_hh\n",
    "        self.W_hy -= learning_rate * dW_hy\n",
    "        self.b_h -= learning_rate * db_h\n",
    "        self.b_y -= learning_rate * db_y\n",
    "\n",
    "rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n",
    "for _ in range(1000):\n",
    "    output = rnn.forward(input_sequence)\n",
    "    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n",
    "    # print(output[\"y_pred\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Calculator\n",
    "\n",
    "position = 2, d_model = 8\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pos_encoding(position: int, d_model: int):\n",
    "    # Your code here\n",
    "    \n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    pos_encoding = np.float16(pos_encoding)\n",
    "\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Multi-Head Attention\n",
    "import numpy as np\n",
    "\n",
    "m, n = 4, 4 \n",
    "n_heads = 2 \n",
    "np.random.seed(42) \n",
    "X = np.arange(m*n).reshape(m,n)\n",
    "X = np.random.permutation(X.flatten()).reshape(m, n)\n",
    "W_q = np.random.randint(0,4,size=(n,n))\n",
    "W_k = np.random.randint(0,5,size=(n,n))\n",
    "W_v = np.random.randint(0,6,size=(n,n))\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v) \n",
    "\n",
    "def softmax(X):\n",
    "    x_max = np.max(X, axis=-1, keepdims=True)  # для стабильности\n",
    "    return np.exp(X - x_max) / np.sum(np.exp(X - x_max), axis=-1, keepdims=True)\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    Q = X @ W_q\n",
    "    K = X @ W_k\n",
    "    V = X @ W_v\n",
    "    return Q, K, V\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q.shape = (seq_len, d_model)\n",
    "    K.shape = (seq_len, d_model)\n",
    "    V.shape = (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    d_model = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_model)\n",
    "    soft_scores = softmax(scores)\n",
    "    scores = np.matmul(soft_scores, V)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def multi_head_attention(Q, K, V, n_heads):\n",
    "    seq_len, d_model = Q.shape\n",
    "    assert d_model % n_heads == 0\n",
    "    d_k = d_model // n_heads\n",
    "    \"\"\"\n",
    "    Каждая голова должна работать со всеми токенами, но с меньшей размерностью признаков\n",
    "    Q = np.array([\n",
    "        [1, 2, 3, 4],\n",
    "        [5, 6, 7, 8],\n",
    "        [9, 10, 11, 12]\n",
    "    ])  # Q.shape = (3, 4)\n",
    "\n",
    "    n_heads = 2\n",
    "    d_k = Q.shape[1] // n_heads\n",
    "\n",
    "    Q_h = Q.reshape(3, 2, 2).transpose(1, 0, 2)\n",
    "    print(Q_h.shape)\n",
    "\n",
    "    Q_h:\n",
    "    [\n",
    "        [[ 1, 2], [5, 6], [9, 10]],  # Первая голова\n",
    "        [[ 3, 4], [7, 8], [11, 12]]  # Вторая голова\n",
    "    ]\n",
    "    \"\"\"\n",
    "    Q_h = Q.reshape(seq_len, n_heads, -1).transpose(1, 0, 2)\n",
    "    K_h = K.reshape(seq_len, n_heads, -1).transpose(1, 0, 2)\n",
    "    V_h = V.reshape(seq_len, n_heads, -1).transpose(1, 0, 2)\n",
    "    full_score = np.array([self_attention(q, k, v) for q, k, v in zip(Q_h, K_h, V_h)])\n",
    "    # Чтобы сконкатить обратно, нужно сделать тот же порядок действий\n",
    "    attn = full_score.transpose(1, 0, 2).reshape(seq_len, d_k * n_heads)\n",
    "    \n",
    "    return attn\n",
    "\n",
    "multi_head_attention(Q, K, V, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Masked Self-Attention\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) \n",
    "X = np.arange(48).reshape(6,8)\n",
    "X = np.random.permutation(X.flatten()).reshape(6, 8) \n",
    "mask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\n",
    "W_q = np.random.randint(0,4,size=(8,8))\n",
    "W_k = np.random.randint(0,5,size=(8,8))\n",
    "W_v = np.random.randint(0,6,size=(8,8)) \n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v) \n",
    "\n",
    "def softmax(X):\n",
    "    x_max = np.max(X, axis=-1, keepdims=True)  # для стабильности\n",
    "    return np.exp(X - x_max) / np.sum(np.exp(X - x_max), axis=-1, keepdims=True)\n",
    "\n",
    "def self_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Q.shape = (seq_len, d_model)\n",
    "    K.shape = (seq_len, d_model)\n",
    "    V.shape = (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    d_model = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_model)\n",
    "    if mask is not None:\n",
    "        scores += mask\n",
    "    soft_scores = softmax(scores)\n",
    "    scores = np.matmul(soft_scores, V)\n",
    "    return scores\n",
    "\n",
    "def compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute Query (Q), Key (K), and Value (V) matrices.\n",
    "    \"\"\"\n",
    "    return np.dot(X, W_q), np.dot(X, W_k), np.dot(X, W_v)\n",
    "\n",
    "def masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute masked self-attention.\n",
    "    \"\"\"\n",
    "    attn = self_attention(Q, K, V, mask)\n",
    "    return attn\n",
    "\n",
    "print(masked_attention(Q, K, V, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Polynomial Features\n",
    "\n",
    "X = np.array([[2, 3],\n",
    "            [3, 4],\n",
    "            [5, 6]])\n",
    "degree = 2\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def polynomial_features(X, degree):\n",
    "    if X.ndim == 1:\n",
    "        X = X[:, np.newaxis]\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    comb = [combinations_with_replacement(range(n_features), d) for d in range(1, degree + 1)]\n",
    "    comb = [item for sublist in comb for item in sublist]\n",
    "    \n",
    "    poly_features = [np.prod(X[:, c], axis=1) for c in comb]\n",
    "    poly_features = np.column_stack(poly_features)\n",
    "    \n",
    "    return np.hstack([np.ones((n_samples, 1)), poly_features])\n",
    "\n",
    "polynomial_features(X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Ranking\n",
    "\n",
    "corpus = [['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']]\n",
    "query = ['the', 'cat']\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def compute_term_freq(document, term):\n",
    "    return Counter(document).get(term, 0)\n",
    "\n",
    "def avdl(corpus):\n",
    "    total_words = sum(len(doc) for doc in corpus)\n",
    "    return total_words / len(corpus)\n",
    "\n",
    "def compute_df_term(corpus):\n",
    "    doc_freqs = Counter()\n",
    "    for doc in corpus:\n",
    "        doc_freqs.update(set(doc))\n",
    "    return doc_freqs\n",
    "\n",
    "def compute_idf_term(N, df):\n",
    "    return np.log((N + 1) / (df + 1))\n",
    "\n",
    "def calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n",
    "    if not corpus or not query:\n",
    "        raise ValueError(\"Corpus and query cannot be empty\")\n",
    "    \n",
    "    N = len(corpus)\n",
    "    avg_doc_len = avdl(corpus)\n",
    "    doc_freqs = compute_df_term(corpus)\n",
    "    idf_cache = {q: compute_idf_term(N, doc_freqs.get(q, 0)) for q in query}\n",
    "    \n",
    "    scores = []\n",
    "    for doc in corpus:\n",
    "        doc_len = len(doc)\n",
    "        term_freq = Counter(doc)\n",
    "        doc_score = 0\n",
    "        for q in query:\n",
    "            if q in term_freq:\n",
    "                tf = term_freq[q]\n",
    "                idf = idf_cache[q]\n",
    "                doc_len_norm = 1 - b + b * (doc_len / avg_doc_len)\n",
    "                term_score = (tf * (k1 + 1)) / (tf + k1 * doc_len_norm)\n",
    "                doc_score += idf * term_score\n",
    "        scores.append(doc_score)\n",
    "    \n",
    "    return np.round(scores, 3)\n",
    "\n",
    "calculate_bm25_scores(corpus, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 Text Generation\n",
    "\n",
    "prompt=\"hello\"\n",
    "n_tokens_to_generate=5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n",
    "    x = wte[inputs] + wpe[range(len(inputs))] \n",
    "    # for block in blocks:\n",
    "    #     x = transformer_block(x, **block, n_head=n_head)\n",
    "    return layer_norm(x, **ln_f) @ wte.T\n",
    "\n",
    "def layer_norm(x, g, b, eps=1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    return g * (x - mean) / np.sqrt(variance + eps) + b\n",
    "\n",
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    for _ in range(n_tokens_to_generate):\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)\n",
    "        next_id = np.argmax(logits[-1])\n",
    "        inputs.append(int(next_id))\n",
    "    return inputs[len(inputs) - n_tokens_to_generate:]\n",
    "\n",
    "def gen_text(prompt: str, n_tokens_to_generate: int = 40):\n",
    "    np.random.seed(42)\n",
    "    encoder, hparams, params = load_encoder_hparams_and_params()\n",
    "    input_ids = encoder.encode(prompt)\n",
    "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "    output_text = encoder.decode(output_ids)\n",
    "    return output_text\n",
    "\n",
    "def load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n",
    "    class DummyBPE:\n",
    "        def __init__(self):\n",
    "            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n",
    "\n",
    "        def encode(self, text: str):\n",
    "            tokens = text.strip().split()\n",
    "            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "        def decode(self, token_ids: list):\n",
    "            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n",
    "            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n",
    "\n",
    "    hparams = {\n",
    "        \"n_ctx\": 1024,\n",
    "        \"n_head\": 12\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"wte\": np.random.rand(3, 10),\n",
    "        \"wpe\": np.random.rand(1024, 10),\n",
    "        \"blocks\": [],\n",
    "        \"ln_f\": {\n",
    "            \"g\": np.ones(10),\n",
    "            \"b\": np.zeros(10),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    encoder = DummyBPE()\n",
    "    return encoder, hparams, params\n",
    "\n",
    "\n",
    "gen_text('hello world', n_tokens_to_generate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это полностью процесс как в gpt2\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def layer_norm(x, g, b, eps=1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    return g * (x - mean) / np.sqrt(variance + eps) + b\n",
    "\n",
    "def linear(x, w, b):\n",
    "    return x @ w + b\n",
    "\n",
    "def ffn(x, c_fc, c_proj):\n",
    "    return linear(gelu(linear(x, **c_fc)), **c_proj)\n",
    "\n",
    "def attention(q, k, v, mask):\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
    "\n",
    "def mha(x, c_attn, c_proj, n_head):\n",
    "    x = linear(x, **c_attn)\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), np.split(x, 3, axis=-1)))\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n",
    "    x = linear(np.hstack(out_heads), **c_proj)\n",
    "    return x\n",
    "\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n",
    "    return x\n",
    "\n",
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n",
    "    x = wte[inputs] + wpe[range(len(inputs))] # position embeddings\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block, n_head=n_head)\n",
    "    return layer_norm(x, **ln_f) @ wte.T\n",
    "\n",
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    for _ in range(n_tokens_to_generate):\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)\n",
    "        next_id = np.argmax(logits[-1])\n",
    "        inputs.append(int(next_id))\n",
    "    return inputs[len(inputs) - n_tokens_to_generate:]\n",
    "\n",
    "def gen_text(prompt: str, n_tokens_to_generate: int = 40):\n",
    "    np.random.seed(42)  # Set the random seed for reproducibility\n",
    "    encoder, hparams, params = load_encoder_hparams_and_params()\n",
    "    input_ids = encoder.encode(prompt)\n",
    "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "    output_text = encoder.decode(output_ids)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Performance Metrics for a Classification Model\n",
    "\n",
    "actual = [1, 0, 1, 0, 1]\n",
    "predicted = [1, 0, 0, 1, 1]\n",
    "\n",
    "def performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for ind in range(len(actual)):\n",
    "        if actual[ind] == 0 and predicted[ind] == 0:\n",
    "            tn += 1\n",
    "        elif actual[ind] == 1 and predicted[ind] == 1:\n",
    "            tp += 1\n",
    "        elif actual[ind] == 1 and predicted[ind] == 0:\n",
    "            fn += 1\n",
    "        elif actual[ind] == 0 and predicted[ind] == 1:\n",
    "            fp += 1\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fn + fp + tn)\n",
    "    \n",
    "    confusion_matrix = [[tp, fn], [fp, tn]]\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    specificity = tn / (tn + fp)\n",
    "    negativePredictive = tn / (tn + fn)\n",
    "\n",
    "    return confusion_matrix, round(accuracy, 3), round(f1, 3), round(specificity, 3), round(negativePredictive, 3)\n",
    "\n",
    "print(performance_metrics(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Layer Normalization for Sequence Data\n",
    "\n",
    "np.random.seed(42); \n",
    "X = np.random.randn(2, 2, 3);\n",
    "gamma = np.ones(3).reshape(1, 1, -1);\n",
    "beta = np.zeros(3).reshape(1, 1, -1);\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n",
    "\t\"\"\"\n",
    "\tPerform Layer Normalization.\n",
    "\t\"\"\"\n",
    "\tmean = np.mean(X, axis=-1, keepdims=True)\n",
    "\tvariance = np.var(X, axis=-1, keepdims=True)\n",
    "\treturn gamma * (X - mean) / np.sqrt(variance + epsilon) + beta\n",
    "\n",
    "\n",
    "layer_normalization(X, gamma, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression with Gradient Descent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def bce(y_true, y_pred):\n",
    "    loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "def train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n",
    "    \"\"\"\n",
    "    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n",
    "    \"\"\"\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X)) # добавляем баес к весам - для этого добавляем в X столбик с 1 - в начало\n",
    "    y = y.reshape(-1, 1)\n",
    "    B = np.zeros((X.shape[1], 1)) # тогда нужно считать всего одну матрицу\n",
    "\n",
    "    losses = []\n",
    "    for _ in range(iterations):\n",
    "        z = X @ B\n",
    "        y_pred = sigmoid(z) \n",
    "        dl_db = X.T @ (y_pred - y) # производная bce loss\n",
    "        B -= learning_rate * dl_db\n",
    "        loss = bce(y, y_pred)\n",
    "        losses.append(round(loss.item(), 4))\n",
    "\n",
    "    return B.flatten().round(4).tolist(), losses\n",
    "\n",
    "\n",
    "X = np.array([[ 0.76743473, -0.23413696, -0.23415337,  1.57921282],\n",
    "    [-1.4123037 ,  0.31424733, -1.01283112, -0.90802408],\n",
    "    [-0.46572975,  0.54256004, -0.46947439, -0.46341769],\n",
    "    [-0.56228753, -1.91328024,  0.24196227, -1.72491783],\n",
    "    [-1.42474819, -0.2257763 ,  1.46564877,  0.0675282 ],\n",
    "    [ 1.85227818, -0.29169375, -0.60063869, -0.60170661],\n",
    "    [ 0.37569802,  0.11092259, -0.54438272, -1.15099358],\n",
    "    [ 0.19686124, -1.95967012,  0.2088636 , -1.32818605],\n",
    "    [ 1.52302986, -0.1382643 ,  0.49671415,  0.64768854],\n",
    "    [-1.22084365, -1.05771093, -0.01349722,  0.82254491]])\n",
    "y = np.array([1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
    "learning_rate = 1e-3\n",
    "iterations = 10\n",
    "train_logreg(X, y, learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Softmax Regression with Gradient Descent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "def train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n",
    "    \"\"\"\n",
    "    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n",
    "    \"\"\"\n",
    "    n_classes = len(np.unique(y))\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))  # Теперь X имеет размер (m, n + 1).\n",
    "    y_one_hot = np.eye(n_classes)[y] # Преобразуем вектор меток y в one-hot представление для работы с Softmax\n",
    "    B = np.zeros((X.shape[1], n_classes)) \n",
    "\n",
    "    losses = []\n",
    "    for _ in range(iterations):\n",
    "        z = X @ B\n",
    "        y_pred = softmax(z)   (X.shape[0], n_classes)\n",
    "        print(y_pred.shape)\n",
    "        \n",
    "        dl_db = (X.T @ (y_pred - y_one_hot)) # \n",
    "        B -= learning_rate * dl_db\n",
    "        loss = cross_entropy(y_one_hot, y_pred)\n",
    "        losses.append(round(loss.item(), 4))\n",
    "\n",
    "    return np.round(B.T, 4).tolist(), losses\n",
    "\n",
    "print(train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], \n",
    "                                [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], \n",
    "                                [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], \n",
    "                                [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], \n",
    "                                [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], \n",
    "                                [0.5482, 0.3315, 0.1067, 0.3069, -0.3755],\n",
    "                                [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], \n",
    "                                [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], \n",
    "                                [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], \n",
    "                                [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve 4x^2 - 5x + 1.5 = 0 with gradient descent\n",
    "# 1. возводим в квадрат, потому что мы должны найти градиент - (4x^2 - 5x + 1.5)^2 = 0\n",
    "# 2. derivative: 2 * (4x^2 − 5x + 1.5) * (8x − 5)\n",
    "\n",
    "def grad(x):\n",
    "    return 2 * (4 * x * x - 5 * x + 1.5) * (8 * x - 5)\n",
    "\n",
    "def solve(iterations: int = 1000, eps: float = 1e-2, learning_rate: float = 1e-2):\n",
    "    x0 = 0.0\n",
    "    it = 0\n",
    "    xs = []\n",
    "    while it < iterations:\n",
    "        if abs(grad(x0)) < eps:\n",
    "            break\n",
    "        x0 -= learning_rate * grad(x0)\n",
    "        it += 1\n",
    "        xs.append(x0)\n",
    "    return xs\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a, b, c = 4, -5, 1.5\n",
    "\n",
    "x = np.linspace(-1, 2, 500)\n",
    "y = a * x**2 + b * x + c\n",
    "\n",
    "roots = np.roots([a, b, c])\n",
    "solution_x_0 = roots[0]\n",
    "solution_x_1 = roots[1]\n",
    "solution_y_0 = a * solution_x_0**2 + b * solution_x_0 + c\n",
    "\n",
    "# Массив точек p (пример) - только координаты x\n",
    "p = np.array(solve())\n",
    "p_y = a * p**2 + b * p + c\n",
    "\n",
    "# Построение графика\n",
    "plt.plot(x, y, label='4x² - 5x + 1.5', color='royalblue')\n",
    "plt.scatter(p, p_y, color='crimson', label='Точки p', zorder=3)\n",
    "\n",
    "# Добавление перпендикулярной линии к решению уравнения\n",
    "plt.axvline(solution_x_0, color='green', linestyle='--', label=f'Решение x={solution_x_0:.2f}')\n",
    "plt.axvline(solution_x_1, color='green', linestyle='--', label=f'Решение x={solution_x_1:.2f}')\n",
    "# Добавление сетки, легенды и заголовков\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.axhline(0, color='black', lw=0.8)\n",
    "plt.axvline(0, color='black', lw=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('График функции 4x² - 5x + 1.5 и точки p')\n",
    "\n",
    "# Показать график\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
