{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"idiot.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[А-яа-я]+[\\w^\\']*|[\\w^\\']*[А-яа-я]+[\\w^\\']*')\n",
    "    tokens = pattern.findall(text.lower())\n",
    "    tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = text[:100000]\n",
    "tokens = tokenize(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, tokens, embedding_dim):\n",
    "        self.tokens = tokens\n",
    "        self.make_vocab()\n",
    "        self.weight_params = self.init_weights(vocab_size=len(self.word2index), embedding_dim=embedding_dim)\n",
    "\n",
    "    def init_weights(self, vocab_size, embedding_dim):\n",
    "        params = {\n",
    "            \"W1\": np.random.uniform(vocab_size, embedding_dim),\n",
    "            \"W2\": np.random.uniform(embedding_dim, vocab_size),\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def make_vocab(self):\n",
    "        \"\"\"\n",
    "        Making vocab for w2v\n",
    "        \"\"\"\n",
    "        word_index = {}\n",
    "        index_word = {}\n",
    "        for ind, tok in enumerate(set(self.tokens)):\n",
    "            word_index[tok] = ind\n",
    "            index_word[ind] = tok\n",
    "        self.word2index = word_index\n",
    "        self.index2word = index_word\n",
    "\n",
    "    def concat(self, *iterables):\n",
    "        for iterable in iterables:\n",
    "            yield from iterable\n",
    "\n",
    "    def one_hot_encode(self, id, vocab_size):\n",
    "        res = [0] * vocab_size\n",
    "        res[id] = 1\n",
    "        return res\n",
    "\n",
    "    def generate_training_data(self, window_size: int = 2):\n",
    "        X = []\n",
    "        y = []\n",
    "        n_tokens = len(self.tokens)\n",
    "        \n",
    "        for i in range(n_tokens):\n",
    "            idx = self.concat(\n",
    "                range(max(0, i - window_size), i), \n",
    "                range(i, min(n_tokens, i + window_size + 1))\n",
    "            )\n",
    "            for j in idx:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                X.append(self.one_hot_encode(self.word2index[self.tokens[i]], len(self.word2index)))\n",
    "                y.append(self.one_hot_encode(self.word2index[self.tokens[j]], len(self.word2index)))\n",
    "        \n",
    "        return np.asarray(X), np.asarray(y)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def forward_pass(self, input_batch):\n",
    "        \"\"\"\n",
    "        input_batch: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        hidden = input_batch @ self.weight_params[\"W1\"]  # (batch_size, embedding_dim)\n",
    "        scores = hidden @ self.weight_params[\"W2\"]       # (batch_size, vocab_size)\n",
    "        return scores, self.softmax(scores)\n",
    "    \n",
    "    def backward_pass(self, input_batch, target_batch, predicted_batch, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        input_batch: (batch_size, vocab_size)\n",
    "        target_batch: (batch_size, vocab_size)\n",
    "        predicted_batch: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        batch_size = input_batch.shape[0]\n",
    "        error = predicted_batch - target_batch  # (batch_size, vocab_size)\n",
    "\n",
    "        hidden = input_batch @ self.weight_params[\"W1\"]  # (batch_size, embedding_dim)\n",
    "\n",
    "        dW2 = hidden.T @ error  # (embedding_dim, vocab_size)\n",
    "        dh = error @ self.weight_params[\"W2\"].T  # (batch_size, embedding_dim)\n",
    "        dW1 = input_batch.T @ dh  # (vocab_size, embedding_dim)\n",
    "\n",
    "        self.weight_params[\"W1\"] -= learning_rate * dW1 / batch_size\n",
    "        self.weight_params[\"W2\"] -= learning_rate * dW2 / batch_size\n",
    "\n",
    "    def most_similar(self, word, top_n=5):\n",
    "        if word not in self.word2index:\n",
    "            print(f\"'{word}' not in vocabulary.\")\n",
    "            return\n",
    "\n",
    "        idx = self.word2index[word]\n",
    "        target_vec = self.weight_params[\"W1\"][idx]\n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)\n",
    "\n",
    "        similarities = []\n",
    "        for i in range(len(self.index2word)):\n",
    "            if i == idx:\n",
    "                continue\n",
    "            vec = self.weight_params[\"W1\"][i]\n",
    "            sim = cosine_similarity(target_vec, vec)\n",
    "            similarities.append((self.index2word[i], sim))\n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(X, y, batch_size: int):\n",
    "    for ind in tqdm(range(0, X.shape[0], batch_size), total=X.shape[0] // batch_size):\n",
    "        X_batch = X[ind: ind + batch_size, :]\n",
    "        y_batch = y[ind: ind + batch_size, :]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Word2Vec(tokens=tokens, embedding_dim=100)\n",
    "X, y = wv.generate_training_data(window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_embeddings(model, epoch=None):\n",
    "    W = model.weight_params[\"W1\"]\n",
    "    words = list(model.word2index.keys())[:50]\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    W_pca = pca.fit_transform(W)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, word in enumerate(words):\n",
    "        x, y = W_pca[i]\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x + 0.01, y + 0.01, word, fontsize=9)\n",
    "\n",
    "    title = f\"Word Embeddings at Epoch {epoch}\" if epoch is not None else \"Word Embeddings\"\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(n_epochs, model, X, y, batch_size, learning_rate=0.01, visualize_every: int = 5):\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in batchify(X, y, batch_size):\n",
    "            scores, preds = model.forward_pass(X_batch)\n",
    "\n",
    "            log_preds = np.log(preds + 1e-8)\n",
    "            batch_loss = -np.sum(y_batch * log_preds)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            model.backward_pass(X_batch, y_batch, preds, learning_rate)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs} | Loss: {total_loss:.4f}\")\n",
    "        if (epoch + 1) % visualize_every == 0 or epoch == n_epochs - 1:\n",
    "            visualize_embeddings(model, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "\n",
    "training(n_epochs=n_epochs, model=wv, X=X, y=y, batch_size=batch_size, learning_rate=learning_rate, visualize_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
